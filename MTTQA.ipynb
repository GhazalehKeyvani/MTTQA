{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9R8VCt3aJt4T",
        "k-Ih0795-MuL"
      ],
      "authorship_tag": "ABX9TyN3O5E98nnrDKunvi+BQhGZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GhazalehKeyvani/MTTQA/blob/main/MTTQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1. Setup and Imports\n"
      ],
      "metadata": {
        "id": "9R8VCt3aJt4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain langchain-openai langchain-community langgraph openai chromadb pandas numpy python-dotenv tiktoken pydantic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "4P94YB7eNCiC",
        "outputId": "dc19648b-2122-428c-ab5e-e74ce574c787"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.10)\n",
            "Collecting langchain-openai\n",
            "  Using cached langchain_openai-1.1.10-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (1.0.9)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.23.0)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.5.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.12.3)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.10 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.2.15)\n",
            "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n",
            "  Downloading langchain_classic-1.0.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.47)\n",
            "Collecting requests<3.0.0,>=2.32.5 (from langchain-community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.4)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.13.1)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.7.6)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (4.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.0.8)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.13.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Using cached build-1.4.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.41.0)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Using cached onnxruntime-1.24.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.39.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.2)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Using cached pypika-0.51.1-py2.py3-none-any.whl.metadata (51 kB)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.78.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Using cached bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.24.1)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Using cached kubernetes-35.0.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.7)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.26.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: packaging>=24.0 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (26.0)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.30.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: urllib3!=2.6.0,>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting langchain-text-splitters<2.0.0,>=1.1.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community)\n",
            "  Downloading langchain_text_splitters-1.1.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (1.33)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (0.14.1)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph) (1.12.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.12.19)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.6)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.39.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.39.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.39.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.60b1 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (1.4.1)\n",
            "Requirement already satisfied: click>=8.2.1 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (0.0.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.1)\n",
            "Requirement already satisfied: watchfiles>=0.20 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.24.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (0.24.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.10->langchain) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Downloading langchain_openai-1.1.10-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-1.5.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.5/21.5 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.4.0-py3-none-any.whl (24 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading kubernetes-35.0.0-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_classic-1.0.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.24.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.39.1-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.39.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.39.1-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.39.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl (219 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypika-0.51.1-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading langchain_text_splitters-1.1.1-py3-none-any.whl (35 kB)\n",
            "Downloading marshmallow-3.26.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: pypika, durationpy, requests, pyproject_hooks, pybase64, opentelemetry-proto, mypy-extensions, marshmallow, bcrypt, backoff, typing-inspect, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, onnxruntime, build, opentelemetry-semantic-conventions, kubernetes, dataclasses-json, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, langchain-text-splitters, langchain-openai, langchain-classic, langchain-community, chromadb\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: opentelemetry-proto\n",
            "    Found existing installation: opentelemetry-proto 1.38.0\n",
            "    Uninstalling opentelemetry-proto-1.38.0:\n",
            "      Successfully uninstalled opentelemetry-proto-1.38.0\n",
            "  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n",
            "    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.38.0\n",
            "    Uninstalling opentelemetry-exporter-otlp-proto-common-1.38.0:\n",
            "      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.38.0\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.38.0\n",
            "    Uninstalling opentelemetry-api-1.38.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.38.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.59b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.59b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.59b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.38.0\n",
            "    Uninstalling opentelemetry-sdk-1.38.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.38.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-exporter-otlp-proto-common==1.38.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-proto==1.38.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-sdk~=1.38.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-5.0.0 build-1.4.0 chromadb-1.5.1 dataclasses-json-0.6.7 durationpy-0.10 kubernetes-35.0.0 langchain-classic-1.0.1 langchain-community-0.4.1 langchain-openai-1.1.10 langchain-text-splitters-1.1.1 marshmallow-3.26.2 mypy-extensions-1.1.0 onnxruntime-1.24.2 opentelemetry-api-1.39.1 opentelemetry-exporter-otlp-proto-common-1.39.1 opentelemetry-exporter-otlp-proto-grpc-1.39.1 opentelemetry-proto-1.39.1 opentelemetry-sdk-1.39.1 opentelemetry-semantic-conventions-0.60b1 posthog-5.4.0 pybase64-1.4.3 pypika-0.51.1 pyproject_hooks-1.2.0 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "requests"
                ]
              },
              "id": "bcbe72407f2c4312bd09844b93f0c2e2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.2.3 langchain-openai langchain-community==0.2.3 langgraph==0.1.1 openai==1.30.1 chromadb==0.5.0 pandas==2.2.2 numpy==1.26.4 python-dotenv==1.0.1 tiktoken==0.6.0 pydantic==2.7.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQYxMGzQjvZM",
        "outputId": "2ed3ed4b-dbc4-4015-e3a4-421ed0653e96",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.2.3\n",
            "  Using cached langchain-0.2.3-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (1.1.10)\n",
            "Collecting langchain-community==0.2.3\n",
            "  Using cached langchain_community-0.2.3-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langgraph==0.1.1\n",
            "  Using cached langgraph-0.1.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting openai==1.30.1\n",
            "  Using cached openai-1.30.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting chromadb==0.5.0\n",
            "  Using cached chromadb-0.5.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting python-dotenv==1.0.1\n",
            "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting tiktoken==0.6.0\n",
            "  Using cached tiktoken-0.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting pydantic==2.7.1\n",
            "  Using cached pydantic-2.7.1-py3-none-any.whl.metadata (107 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.2.3) (6.0.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain==0.2.3) (2.0.47)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.2.3) (3.13.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.0 (from langchain==0.2.3)\n",
            "  Using cached langchain_core-0.2.43-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain==0.2.3)\n",
            "  Using cached langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.2.3)\n",
            "  Using cached langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain==0.2.3) (2.32.5)\n",
            "Collecting tenacity<9.0.0,>=8.1.0 (from langchain==0.2.3)\n",
            "  Using cached tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.2.3) (0.6.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.30.1) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.30.1) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.30.1) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai==1.30.1) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai==1.30.1) (4.67.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.12/dist-packages (from openai==1.30.1) (4.15.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb==0.5.0) (1.4.0)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb==0.5.0)\n",
            "  Using cached chroma-hnswlib-0.7.3.tar.gz (31 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.12/dist-packages (from chromadb==0.5.0) (0.133.0)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.5.0) (0.41.0)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb==0.5.0) (5.4.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb==0.5.0) (1.24.2)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb==0.5.0) (1.39.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb==0.5.0) (1.39.1)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb==0.5.0)\n",
            "  Using cached opentelemetry_instrumentation_fastapi-0.60b1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb==0.5.0) (1.39.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb==0.5.0) (0.22.2)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb==0.5.0) (0.51.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb==0.5.0) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb==0.5.0) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb==0.5.0) (1.78.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb==0.5.0) (5.0.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb==0.5.0) (0.24.1)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb==0.5.0) (35.0.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb==0.5.0) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb==0.5.0) (3.11.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2) (2025.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken==0.6.0) (2025.11.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic==2.7.1) (0.7.0)\n",
            "Collecting pydantic-core==2.18.2 (from pydantic==2.7.1)\n",
            "  Using cached pydantic_core-2.18.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
            "INFO: pip is looking at multiple versions of langchain-openai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-openai\n",
            "  Using cached langchain_openai-1.1.9-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Using cached langchain_openai-1.1.8-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Using cached langchain_openai-1.1.7-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-1.1.6-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-1.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-1.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-1.1.3-py3-none-any.whl.metadata (2.6 kB)\n",
            "INFO: pip is still looking at multiple versions of langchain-openai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached langchain_openai-1.1.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-1.1.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-1.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-1.0.3-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-1.0.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached langchain_openai-1.0.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Using cached langchain_openai-1.0.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Using cached langchain_openai-0.3.35-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Using cached langchain_openai-0.3.34-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Using cached langchain_openai-0.3.33-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Using cached langchain_openai-0.3.32-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Using cached langchain_openai-0.3.31-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Using cached langchain_openai-0.3.30-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Using cached langchain_openai-0.3.29-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Using cached langchain_openai-0.3.28-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.27-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.26-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.25-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.24-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.23-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.22-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.21-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.20-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.19-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.18-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.17-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.16-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.15-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.14-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.13-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.12-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.11-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.10-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.9-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.7-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.6-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.5-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Using cached langchain_openai-0.3.3-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Using cached langchain_openai-0.3.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Using cached langchain_openai-0.3.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Using cached langchain_openai-0.3.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Using cached langchain_openai-0.2.14-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Using cached langchain_openai-0.2.13-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Using cached langchain_openai-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Using cached langchain_openai-0.2.11-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Using cached langchain_openai-0.2.10-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.2.9-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.2.8-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.2.7-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.2.6-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.2.5-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.2.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.2.3-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.2.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.1.25-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.1.24-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.1.23-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.1.22-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.1.20-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.1.19-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Using cached langchain_openai-0.1.17-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.16-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.15-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.14-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.13-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.12-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.11-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.10-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.7-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.6-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.5-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.3-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.1.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.0.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.0.7-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.0.6-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.0.5-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.0.4-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.0.3-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Using cached langchain_openai-0.0.2.post1-py3-none-any.whl.metadata (2.4 kB)\n",
            "  Using cached langchain_openai-0.0.2-py3-none-any.whl.metadata (570 bytes)\n",
            "\u001b[31mERROR: Cannot install langchain-community==0.2.3, langchain-openai==0.0.2, langchain-openai==0.0.2.post1, langchain-openai==0.0.3, langchain-openai==0.0.4, langchain-openai==0.0.5, langchain-openai==0.0.6, langchain-openai==0.0.7, langchain-openai==0.0.8, langchain-openai==0.1.1, langchain-openai==0.1.10, langchain-openai==0.1.11, langchain-openai==0.1.12, langchain-openai==0.1.13, langchain-openai==0.1.14, langchain-openai==0.1.15, langchain-openai==0.1.16, langchain-openai==0.1.17, langchain-openai==0.1.19, langchain-openai==0.1.2, langchain-openai==0.1.20, langchain-openai==0.1.22, langchain-openai==0.1.23, langchain-openai==0.1.24, langchain-openai==0.1.25, langchain-openai==0.1.3, langchain-openai==0.1.4, langchain-openai==0.1.5, langchain-openai==0.1.6, langchain-openai==0.1.7, langchain-openai==0.1.8, langchain-openai==0.1.9, langchain-openai==0.2.0, langchain-openai==0.2.1, langchain-openai==0.2.10, langchain-openai==0.2.11, langchain-openai==0.2.12, langchain-openai==0.2.13, langchain-openai==0.2.14, langchain-openai==0.2.2, langchain-openai==0.2.3, langchain-openai==0.2.4, langchain-openai==0.2.5, langchain-openai==0.2.6, langchain-openai==0.2.7, langchain-openai==0.2.8, langchain-openai==0.2.9, langchain-openai==0.3.0, langchain-openai==0.3.1, langchain-openai==0.3.10, langchain-openai==0.3.11, langchain-openai==0.3.12, langchain-openai==0.3.13, langchain-openai==0.3.14, langchain-openai==0.3.15, langchain-openai==0.3.16, langchain-openai==0.3.17, langchain-openai==0.3.18, langchain-openai==0.3.19, langchain-openai==0.3.2, langchain-openai==0.3.20, langchain-openai==0.3.21, langchain-openai==0.3.22, langchain-openai==0.3.23, langchain-openai==0.3.24, langchain-openai==0.3.25, langchain-openai==0.3.26, langchain-openai==0.3.27, langchain-openai==0.3.28, langchain-openai==0.3.29, langchain-openai==0.3.3, langchain-openai==0.3.30, langchain-openai==0.3.31, langchain-openai==0.3.32, langchain-openai==0.3.33, langchain-openai==0.3.34, langchain-openai==0.3.35, langchain-openai==0.3.4, langchain-openai==0.3.5, langchain-openai==0.3.6, langchain-openai==0.3.7, langchain-openai==0.3.8, langchain-openai==0.3.9, langchain-openai==1.0.0, langchain-openai==1.0.1, langchain-openai==1.0.2, langchain-openai==1.0.3, langchain-openai==1.1.0, langchain-openai==1.1.1, langchain-openai==1.1.10, langchain-openai==1.1.2, langchain-openai==1.1.3, langchain-openai==1.1.4, langchain-openai==1.1.5, langchain-openai==1.1.6, langchain-openai==1.1.7, langchain-openai==1.1.8, langchain-openai==1.1.9, langchain==0.2.3, langgraph==0.1.1, openai==1.30.1 and tiktoken==0.6.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 1.1.10 depends on langchain-core<2.0.0 and >=1.2.13\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 1.1.9 depends on langchain-core<2.0.0 and >=1.2.11\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 1.1.8 depends on langchain-core<2.0.0 and >=1.2.9\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 1.1.7 depends on langchain-core<2.0.0 and >=1.2.6\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 1.1.6 depends on langchain-core<2.0.0 and >=1.2.2\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 1.1.5 depends on langchain-core<2.0.0 and >=1.2.2\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 1.1.4 depends on langchain-core<2.0.0 and >=1.2.1\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 1.1.3 depends on langchain-core<2.0.0 and >=1.1.3\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 1.1.2 depends on langchain-core<2.0.0 and >=1.1.3\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 1.1.1 depends on langchain-core<2.0.0 and >=1.1.1\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 1.1.0 depends on langchain-core<2.0.0 and >=1.1.0\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 1.0.3 depends on langchain-core<2.0.0 and >=1.0.2\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 1.0.2 depends on langchain-core<2.0.0 and >=1.0.2\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 1.0.1 depends on langchain-core<2.0.0 and >=1.0.0\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 1.0.0 depends on langchain-core<2.0.0 and >=1.0.0\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.35 depends on langchain-core<1.0.0 and >=0.3.78\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.34 depends on langchain-core<2.0.0 and >=0.3.77\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.33 depends on langchain-core<1.0.0 and >=0.3.76\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.32 depends on langchain-core<1.0.0 and >=0.3.74\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.31 depends on langchain-core<1.0.0 and >=0.3.74\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.30 depends on langchain-core<1.0.0 and >=0.3.74\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.29 depends on langchain-core<1.0.0 and >=0.3.74\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.28 depends on langchain-core<1.0.0 and >=0.3.68\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.27 depends on langchain-core<1.0.0 and >=0.3.66\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.26 depends on langchain-core<1.0.0 and >=0.3.66\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.25 depends on langchain-core<1.0.0 and >=0.3.66\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.24 depends on langchain-core<1.0.0 and >=0.3.65\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.23 depends on langchain-core<1.0.0 and >=0.3.65\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.22 depends on langchain-core<1.0.0 and >=0.3.64\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.21 depends on langchain-core<1.0.0 and >=0.3.64\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.20 depends on langchain-core<1.0.0 and >=0.3.64\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.19 depends on langchain-core<1.0.0 and >=0.3.63\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.18 depends on langchain-core<1.0.0 and >=0.3.61\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.17 depends on langchain-core<1.0.0 and >=0.3.59\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.16 depends on langchain-core<1.0.0 and >=0.3.58\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.15 depends on langchain-core<1.0.0 and >=0.3.56\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.14 depends on langchain-core<1.0.0 and >=0.3.53\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.13 depends on langchain-core<1.0.0 and >=0.3.52\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.12 depends on langchain-core<1.0.0 and >=0.3.49\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.11 depends on langchain-core<1.0.0 and >=0.3.49\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.10 depends on langchain-core<1.0.0 and >=0.3.48\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.9 depends on langchain-core<1.0.0 and >=0.3.45\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.8 depends on langchain-core<1.0.0 and >=0.3.42\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.7 depends on langchain-core<1.0.0 and >=0.3.39\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.6 depends on langchain-core<1.0.0 and >=0.3.35\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.5 depends on langchain-core<1.0.0 and >=0.3.34\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.4 depends on langchain-core<1.0.0 and >=0.3.34\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.3 depends on langchain-core<0.4.0 and >=0.3.33\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.2 depends on langchain-core<0.4.0 and >=0.3.31\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.1 depends on langchain-core<0.4.0 and >=0.3.30\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.3.0 depends on langchain-core<0.4.0 and >=0.3.29\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.2.14 depends on langchain-core<0.4.0 and >=0.3.27\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.2.13 depends on langchain-core<0.4.0 and >=0.3.27\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.2.12 depends on langchain-core<0.4.0 and >=0.3.21\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.2.11 depends on langchain-core<0.4.0 and >=0.3.21\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.2.10 depends on langchain-core<0.4.0 and >=0.3.21\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.2.9 depends on langchain-core<0.4.0 and >=0.3.17\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.2.8 depends on langchain-core<0.4.0 and >=0.3.17\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.2.7 depends on langchain-core<0.4.0 and >=0.3.16\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.2.6 depends on langchain-core<0.4.0 and >=0.3.15\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.2.5 depends on langchain-core<0.4.0 and >=0.3.15\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.2.4 depends on langchain-core<0.4.0 and >=0.3.13\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.2.3 depends on langchain-core<0.4.0 and >=0.3.12\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.2.2 depends on langchain-core<0.4.0 and >=0.3.9\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.2.1 depends on langchain-core<0.4 and >=0.3\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.2.0 depends on langchain-core<0.4 and >=0.3\n",
            "    The user requested openai==1.30.1\n",
            "    langchain-openai 0.1.25 depends on openai<2.0.0 and >=1.40.0\n",
            "    The user requested openai==1.30.1\n",
            "    langchain-openai 0.1.24 depends on openai<2.0.0 and >=1.40.0\n",
            "    The user requested openai==1.30.1\n",
            "    langchain-openai 0.1.23 depends on openai<2.0.0 and >=1.40.0\n",
            "    The user requested openai==1.30.1\n",
            "    langchain-openai 0.1.22 depends on openai<2.0.0 and >=1.40.0\n",
            "    The user requested openai==1.30.1\n",
            "    langchain-openai 0.1.20 depends on openai<2.0.0 and >=1.32.0\n",
            "    The user requested openai==1.30.1\n",
            "    langchain-openai 0.1.19 depends on openai<2.0.0 and >=1.32.0\n",
            "    The user requested openai==1.30.1\n",
            "    langchain-openai 0.1.17 depends on openai<2.0.0 and >=1.32.0\n",
            "    The user requested openai==1.30.1\n",
            "    langchain-openai 0.1.16 depends on openai<2.0.0 and >=1.32.0\n",
            "    The user requested openai==1.30.1\n",
            "    langchain-openai 0.1.15 depends on openai<2.0.0 and >=1.32.0\n",
            "    The user requested openai==1.30.1\n",
            "    langchain-openai 0.1.14 depends on openai<2.0.0 and >=1.32.0\n",
            "    The user requested openai==1.30.1\n",
            "    langchain-openai 0.1.13 depends on openai<2.0.0 and >=1.32.0\n",
            "    The user requested openai==1.30.1\n",
            "    langchain-openai 0.1.12 depends on openai<2.0.0 and >=1.32.0\n",
            "    The user requested openai==1.30.1\n",
            "    langchain-openai 0.1.11 depends on openai<2.0.0 and >=1.32.0\n",
            "    The user requested tiktoken==0.6.0\n",
            "    langchain-openai 0.1.10 depends on tiktoken<1 and >=0.7\n",
            "    The user requested tiktoken==0.6.0\n",
            "    langchain-openai 0.1.9 depends on tiktoken<1 and >=0.7\n",
            "    The user requested tiktoken==0.6.0\n",
            "    langchain-openai 0.1.8 depends on tiktoken<1 and >=0.7\n",
            "    The user requested tiktoken==0.6.0\n",
            "    langchain-openai 0.1.7 depends on tiktoken<1 and >=0.7\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.1.6 depends on langchain-core<0.2.0 and >=0.1.46\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.1.5 depends on langchain-core<0.2.0 and >=0.1.46\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.1.4 depends on langchain-core<0.2.0 and >=0.1.46\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.1.3 depends on langchain-core<0.2.0 and >=0.1.42\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.1.2 depends on langchain-core<0.2.0 and >=0.1.41\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.1.1 depends on langchain-core<0.2.0 and >=0.1.33\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.0.8 depends on langchain-core<0.2.0 and >=0.1.27\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.0.7 depends on langchain-core<0.2.0 and >=0.1.26\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.0.6 depends on langchain-core<0.2 and >=0.1.16\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.0.5 depends on langchain-core<0.2 and >=0.1.16\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.0.4 depends on langchain-core<0.2 and >=0.1.16\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.0.3 depends on langchain-core<0.2 and >=0.1.13\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.0.2.post1 depends on langchain-core<0.2 and >=0.1.7\n",
            "    langchain 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langchain-community 0.2.3 depends on langchain-core<0.3.0 and >=0.2.0\n",
            "    langgraph 0.1.1 depends on langchain-core<0.3 and >=0.2\n",
            "    langchain-openai 0.0.2 depends on langchain-core<0.2 and >=0.1.7\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rouge-score nltk sentence-transformers scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_CHGgaekOmSK",
        "outputId": "550dda19-7c90-4a1d-eaee-2ddc4988fb62"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.3)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (5.0.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.10.0+cpu)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.24.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.5.4)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.24.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (0.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: typer>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (0.24.1)\n",
            "Requirement already satisfied: rich>=12.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (13.9.4)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (0.0.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (0.1.2)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=24af919fb3450c8c789f15a9e490589dbc284f436c7d84b3635ac254a4cabc2b\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import TypedDict, List, Dict, Any, Optional\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# LangChain / LangGraph\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.tools import tool, Tool, BaseTool\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# Pydantic for tool schemas\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# (Optional) if you need AgentExecutor later\n",
        "#from langchain.agents import AgentExecutor"
      ],
      "metadata": {
        "id": "d3wsGXEmoYL6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Create path\n",
        "data_dir = Path(\"./TAT_QA\")\n",
        "data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Download Files: train، dev و test\n",
        "base_url = \"https://huggingface.co/datasets/TableQAKit/TAT-QA/resolve/main/\"\n",
        "files = [\"tatqa_dataset_train.json\", \"tatqa_dataset_dev.json\", \"tatqa_dataset_test.json\"]\n",
        "\n",
        "for file in files:\n",
        "    url = base_url + file\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        with open(data_dir / file, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"✅ {file} Downloaded\")\n",
        "    else:\n",
        "        print(f\"❌ Download error {file}\")\n",
        "\n",
        "# Uploade data\n",
        "def load_tatqa(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "train_data = load_tatqa(data_dir / \"tatqa_dataset_train.json\")\n",
        "print(f\"\\n Number of train: {len(train_data)}\")\n",
        "print(f\"A sample structure \\n{json.dumps(train_data[0], indent=2)[:500]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZU8NjEg23XW",
        "outputId": "2ac0e593-76f2-4f32-fabd-c17581ad49fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ tatqa_dataset_train.json Downloaded\n",
            "✅ tatqa_dataset_dev.json Downloaded\n",
            "✅ tatqa_dataset_test.json Downloaded\n",
            "\n",
            " Number of train: 2204\n",
            "A sample structure \n",
            "{\n",
            "  \"table\": {\n",
            "    \"uid\": \"2afe0a52e5a2dec3bb6c67450fcd5222\",\n",
            "    \"table\": [\n",
            "      [\n",
            "        \"\",\n",
            "        \"F19\",\n",
            "        \"F18\",\n",
            "        \"\",\n",
            "        \"CHANGE\"\n",
            "      ],\n",
            "      [\n",
            "        \"$ MILLION\",\n",
            "        \"53 WEEKS\",\n",
            "        \"52 WEEKS\",\n",
            "        \"CHANGE\",\n",
            "        \"NORMALISED\"\n",
            "      ],\n",
            "      [\n",
            "        \"Sales\",\n",
            "        \"3,797\",\n",
            "        \"3,566\",\n",
            "        \"6.5%\",\n",
            "        \"4.2%\"\n",
            "      ],\n",
            "      [\n",
            "        \"LBITDA before\",\n",
            "        \"\",\n",
            "        \"\",\n",
            "        \"\",\n",
            "        \"\"\n",
            "      ],\n",
            "      [\n",
            "        \"significant i...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "load_dotenv()\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "if not OPENAI_API_KEY:\n",
        "    raise ValueError(\"Please set OPENAI_API_KEY in .env file\")\n",
        "\n",
        "# Configuration\n",
        "MODEL_NAME = \"gpt-4o-mini\"  # Use a cost-effective model; replace with open-source if needed\n",
        "TEMPERATURE = 0\n",
        "VECTOR_STORE_PATH = \"./chroma_db\"\n",
        "TATQA_PATH = \"./TAT_QA/\"  # Path to dataset (should contain train.json, dev.json, test.json)\n",
        "\n",
        "# We'll use OpenAI embeddings for text retrieval\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Instantiate the LLM\n",
        "llm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE)"
      ],
      "metadata": {
        "id": "69yegRzhoaV7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load and Preprocess TAT-QA Data"
      ],
      "metadata": {
        "id": "fpYVkua6Kzca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_tatqa_split(split: str):\n",
        "    with open(os.path.join(TATQA_PATH, f\"{split}.json\"), \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "# Load a small subset for demonstration (use full for evaluation)\n",
        "train_data = load_tatqa_split(\"tatqa_dataset_train\")[:5]   # Take first 5 for quick demo\n",
        "dev_data = load_tatqa_split(\"tatqa_dataset_dev\")[:2]\n",
        "test_data = load_tatqa_split(\"tatqa_dataset_test\")[:2]\n",
        "\n",
        "print(f\"Train samples: {len(train_data)}\")\n",
        "print(f\"Dev samples: {len(dev_data)}\")\n",
        "print(f\"Test samples: {len(test_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phsn-EhbqhU0",
        "outputId": "388e54f8-faca-4cad-b0d7-c49df8ef5977"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 5\n",
            "Dev samples: 2\n",
            "Test samples: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Inspect a sample"
      ],
      "metadata": {
        "id": "YltTVvfhK9GR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = train_data[0]\n",
        "print(\"Table UID:\", sample[\"table\"][\"uid\"])\n",
        "print(\"Table:\\n\", pd.DataFrame(sample[\"table\"][\"table\"]))\n",
        "print(\"\\nParagraphs:\")\n",
        "for p in sample[\"paragraphs\"]:\n",
        "    print(f\"  [{p['order']}] {p['text']}\")\n",
        "print(\"\\nQuestions:\")\n",
        "for q in sample[\"questions\"]:\n",
        "    print(f\"  Q: {q['question']}  A: {q['answer']} (type: {q['answer_type']})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWeNsWElwpmI",
        "outputId": "e74b2d0c-8008-4bd5-f6f5-0e4c575be61a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table UID: 2afe0a52e5a2dec3bb6c67450fcd5222\n",
            "Table:\n",
            "                                 0         1         2          3           4\n",
            "0                                       F19       F18                 CHANGE\n",
            "1                       $ MILLION  53 WEEKS  52 WEEKS     CHANGE  NORMALISED\n",
            "2                           Sales     3,797     3,566       6.5%        4.2%\n",
            "3                   LBITDA before                                           \n",
            "4               significant items       (5)      (30)    (82.2)%     (88.7)%\n",
            "5   Depreciation and amortisation      (80)      (80)       0.7%        0.7%\n",
            "6   LBIT before significant items      (85)     (110)    (22.2)%     (24.0)%\n",
            "7               Significant items     (371)         –       n.m.        n.m.\n",
            "8    LBIT after significant items     (456)     (110)     315.5%      313.7%\n",
            "9                Gross margin (%)      31.1      31.7   (59) bps    (49) bps\n",
            "10     Cost of doing business (%)      33.4      34.8  (142) bps   (132) bps\n",
            "11            LBIT 2 to sales (%)     (2.3)     (3.1)     83 bps      84 bps\n",
            "12   Sales per square metre ($)$)     3,629     3,369       7.7%        5.4%\n",
            "13                 Funds employed       204       502    (59.4)%            \n",
            "14                       ROFE (%)    (23.0)    (23.3)     24 bps      77 bps\n",
            "\n",
            "Paragraphs:\n",
            "  [1] Funds employed declined primarily due to significant items provisions. Inventory quality has improved as a result of solid sales and improved apparel sell‐through in H2.\n",
            "  [2] In F20, BIG W will focus on creating a sustainable business that is simpler to operate, and continue providing customers with low prices and more convenient, connected solutions in‐store and online.\n",
            "\n",
            "Questions:\n",
            "  Q: What is the focus of BIG W in F20?  A: ['In F20, BIG W will focus on creating a sustainable business that is simpler to operate, and continue providing customers with low prices and more convenient, connected solutions in‐store and online.'] (type: span)\n",
            "  Q: What is the gross margin in F19?  A: ['31.1'] (type: span)\n",
            "  Q: Why did the funds employed declined?  A: ['Funds employed declined primarily due to significant items provisions.'] (type: span)\n",
            "  Q: What is the nominal difference between the sales in F19 and F18?  A: 231 (type: arithmetic)\n",
            "  Q: What is the average cost of doing business (%) for both F19 and F18?  A: 34.1 (type: arithmetic)\n",
            "  Q: What is the nominal difference between sales per square metre for F19 and F18?  A: 260 (type: arithmetic)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Build Vector Store for Paragraphs\n",
        "\n",
        "\n",
        "We will index all paragraphs from the dataset for retrieval. For simplicity, we only index the training set paragraphs."
      ],
      "metadata": {
        "id": "aHdQ9FYQ9cr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_paragraphs(data):\n",
        "    docs = []\n",
        "    ids = []\n",
        "    for sample in data:\n",
        "        for p in sample[\"paragraphs\"]:\n",
        "            docs.append(p[\"text\"])\n",
        "            ids.append(p[\"uid\"])\n",
        "    return docs, ids\n",
        "\n",
        "train_paragraphs, train_para_ids = prepare_paragraphs(train_data)\n",
        "print(f\"Indexing {len(train_paragraphs)} paragraphs...\")\n",
        "\n",
        "# Create vector store (if not exists)\n",
        "if not os.path.exists(VECTOR_STORE_PATH):\n",
        "    vectorstore = Chroma.from_texts(\n",
        "        texts=train_paragraphs,\n",
        "        embedding=embeddings,\n",
        "        metadatas=[{\"uid\": uid} for uid in train_para_ids],\n",
        "        persist_directory=VECTOR_STORE_PATH\n",
        "    )\n",
        "    vectorstore.persist()\n",
        "else:\n",
        "    vectorstore = Chroma(persist_directory=VECTOR_STORE_PATH, embedding_function=embeddings)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPIWeK1X4PBt",
        "outputId": "f7430aaf-1c18-4077-8737-5505d71affa6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexing 13 paragraphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-12780/4097979710.py:21: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  vectorstore.persist()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Define Tools\n",
        "Each tool will be callable by the agents."
      ],
      "metadata": {
        "id": "ImdnYX9z9wRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tool 1: Table Query Tool\n",
        "def table_query_tool(query: str, table_df: pd.DataFrame) -> str:\n",
        "    \"\"\"\n",
        "    Execute a natural language query on a pandas DataFrame and return result as string.\n",
        "    In a production system, you would use an LLM to convert the query to pandas code.\n",
        "    For this demo, we implement a simple keyword-based cell lookup.\n",
        "    \"\"\"\n",
        "    # Very simple parser: try to find cell by row/column keywords\n",
        "    # For full implementation, use an LLM to generate pandas code.\n",
        "    query_lower = query.lower()\n",
        "    # Try to extract a year (e.g., 2019)\n",
        "    import re\n",
        "    year_match = re.search(r'\\b(20\\d{2})\\b', query)\n",
        "    col = None\n",
        "    if year_match:\n",
        "        col = year_match.group(1)\n",
        "    # Try to extract a row name\n",
        "    row_keywords = ['revenue', 'income', 'profit', 'cost', 'assets']\n",
        "    row = None\n",
        "    for kw in row_keywords:\n",
        "        if kw in query_lower:\n",
        "            row = kw.capitalize()\n",
        "            break\n",
        "    # If both found, try to locate cell\n",
        "    if col and row:\n",
        "        # Assume first row is header\n",
        "        if col in table_df.columns:\n",
        "            mask = table_df.iloc[:, 0].astype(str).str.contains(row, case=False, na=False)\n",
        "            if mask.any():\n",
        "                value = table_df.loc[mask, col].values[0]\n",
        "                return f\"The value for {row} in {col} is {value}.\"\n",
        "    return \"Could not locate exact cell. Please refine your query.\"\n",
        "\n",
        "# Wrap as LangChain tool\n",
        "from langchain_core.tools import BaseTool\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class TableQueryInput(BaseModel):\n",
        "    query: str = Field(description=\"Natural language query about the table\")\n",
        "    table_df: Any = Field(description=\"Pandas DataFrame of the table\")\n",
        "\n",
        "class TableQueryTool(BaseTool):\n",
        "    name: str = \"table_query\"\n",
        "    description: str = \"Query the table to retrieve a specific cell value based on row and column descriptions.\"\n",
        "    args_schema: type[BaseModel] = TableQueryInput\n",
        "\n",
        "    def _run(self, query: str, table_df: pd.DataFrame) -> str:\n",
        "        return table_query_tool(query, table_df)\n",
        "\n",
        "    async def _arun(self, query: str, table_df: pd.DataFrame) -> str:\n",
        "        raise NotImplementedError\n",
        "\n",
        "# Tool 2: Text Retriever Tool\n",
        "def text_retriever_tool(question: str) -> str:\n",
        "    \"\"\"Retrieve relevant paragraphs from the document.\"\"\"\n",
        "    docs = retriever.invoke(question)   # ✅ use invoke\n",
        "    if not docs:\n",
        "        return \"No relevant text found.\"\n",
        "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "text_retriever_tool_obj = Tool(\n",
        "    name=\"text_retriever\",\n",
        "    func=text_retriever_tool,\n",
        "    description=\"Retrieve paragraphs relevant to the question.\"\n",
        ")\n",
        "\n",
        "# Tool 3: Calculator Tool\n",
        "def calculator_tool(expression: str) -> str:\n",
        "    \"\"\"Safely evaluate a mathematical expression.\"\"\"\n",
        "    try:\n",
        "        # Use a safe eval\n",
        "        allowed_names = {\"abs\": abs, \"round\": round, \"min\": min, \"max\": max}\n",
        "        code = compile(expression, \"<string>\", \"eval\")\n",
        "        for name in code.co_names:\n",
        "            if name not in allowed_names:\n",
        "                raise NameError(f\"Use of {name} not allowed\")\n",
        "        result = eval(code, {\"__builtins__\": {}}, allowed_names)\n",
        "        return str(result)\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "calculator_tool_obj = Tool(\n",
        "    name=\"calculator\",\n",
        "    func=calculator_tool,\n",
        "    description=\"Perform arithmetic calculations. Input should be a valid Python expression.\"\n",
        ")\n",
        "\n",
        "# Tool 4: Knowledge Graph Lookup (optional, simplified)\n",
        "def kg_lookup_tool(entity: str) -> str:\n",
        "    \"\"\"Look up an entity in a simple in-memory knowledge graph.\"\"\"\n",
        "    # For demo, return static info; in real system, query a graph DB.\n",
        "    return f\"Found information about {entity}: ...\"\n",
        "\n",
        "kg_tool = Tool(\n",
        "    name=\"kg_lookup\",\n",
        "    func=kg_lookup_tool,\n",
        "    description=\"Look up an entity in the knowledge graph.\"\n",
        ")\n",
        "\n",
        "# Collect all tools\n",
        "tools = [text_retriever_tool_obj, calculator_tool_obj, kg_tool]  # table tool will be called separately with df"
      ],
      "metadata": {
        "id": "EjSV_uSW4OfM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Define Agent Nodes for LangGraph\n",
        "\n",
        "We'll define a state that holds all necessary information."
      ],
      "metadata": {
        "id": "ThNsj_tN94ga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    question: str\n",
        "    table_df: Optional[pd.DataFrame]\n",
        "    table_headers: Optional[List[str]]\n",
        "    table_rows: Optional[List[List[str]]]\n",
        "    paragraphs: List[str]\n",
        "    retrieved_texts: List[str]\n",
        "    retrieved_paragraph_ids: List[int] # Add this line\n",
        "    extracted_numbers: List[float]\n",
        "    plan: List[str]\n",
        "    intermediate_results: Dict[str, Any]\n",
        "    final_answer: Optional[str]\n",
        "    error: Optional[str]\n",
        "\n",
        "# We will create nodes that update the state.\n",
        "\n",
        "# Node 1: Planning Agent\n",
        "def planning_agent(state: AgentState) -> AgentState:\n",
        "    \"\"\"Analyse the question and produce a step-by-step plan.\"\"\"\n",
        "    messages = [\n",
        "        SystemMessage(content=\"You are a planning agent for table-and-text question answering. \"\n",
        "                              \"Given a question, produce a numbered list of steps to answer it. \"\n",
        "                              \"Use the available tools: text_retriever, calculator, kg_lookup, and a table query tool (which you will call later).\"),\n",
        "        HumanMessage(content=state[\"question\"])\n",
        "    ]\n",
        "    response = llm.invoke(messages)\n",
        "    plan = response.content.strip().split(\"\\n\")\n",
        "    state[\"plan\"] = plan\n",
        "    return state\n",
        "\n",
        "def verification_agent(state: AgentState) -> AgentState:\n",
        "    \"\"\"Check the consistency of the answer and produce final answer.\"\"\"\n",
        "    context = f\"Question: {state['question']}\\n\"\n",
        "    if \"table_value\" in state[\"intermediate_results\"]:\n",
        "        context += f\"Table value: {state['intermediate_results']['table_value']}\\n\"\n",
        "    if \"retrieved_texts\" in state:\n",
        "        context += f\"Retrieved texts: {' '.join(state['retrieved_texts'])}\\n\"\n",
        "    if \"calculation_result\" in state[\"intermediate_results\"]:\n",
        "        context += f\"Calculation result: {state['intermediate_results']['calculation_result']}\\n\"\n",
        "\n",
        "    messages = [\n",
        "        SystemMessage(content=\"You are a verification agent. Based on the evidence, provide a concise final answer to the question.\"),\n",
        "        HumanMessage(content=context)\n",
        "    ]\n",
        "    response = llm.invoke(messages)\n",
        "    state[\"final_answer\"] = response.content.strip()\n",
        "    return state\n",
        "\n",
        "# Node 2: Table Parsing Agent\n",
        "def table_parsing_agent(state: AgentState) -> AgentState:\n",
        "    \"\"\"Extract necessary information from the table based on the plan.\"\"\"\n",
        "    if not state[\"table_headers\"] or not state[\"table_rows\"]:\n",
        "        state[\"error\"] = \"No table provided.\"\n",
        "        return state\n",
        "\n",
        "    # Build DataFrame locally (safe, not stored in state)\n",
        "    df = pd.DataFrame(state[\"table_rows\"], columns=state[\"table_headers\"])\n",
        "\n",
        "    # (Rest of your logic unchanged – use df)\n",
        "    import re\n",
        "    years = re.findall(r'\\b(20\\d{2})\\b', state[\"question\"])\n",
        "    if years:\n",
        "        year = years[0]\n",
        "        row_names = df.iloc[:, 0].tolist()\n",
        "        keywords = ['revenue', 'income', 'profit', 'cost', 'assets']\n",
        "        found_row = None\n",
        "        for kw in keywords:\n",
        "            if kw in state[\"question\"].lower():\n",
        "                for rn in row_names:\n",
        "                    if kw in str(rn).lower():\n",
        "                        found_row = rn\n",
        "                        break\n",
        "                if found_row:\n",
        "                    break\n",
        "        if found_row and year in df.columns:\n",
        "            val = df.loc[df.iloc[:, 0] == found_row, year].values[0]\n",
        "            state[\"intermediate_results\"][\"table_value\"] = val\n",
        "            state[\"intermediate_results\"][\"table_row\"] = found_row\n",
        "    return state\n",
        "\n",
        "    # Use the plan to decide what to extract\n",
        "    # For demo, we hardcode a simple extraction: look for a year mentioned\n",
        "    import re\n",
        "    years = re.findall(r'\\b(20\\d{2})\\b', state[\"question\"])\n",
        "    if years:\n",
        "        year = years[0]\n",
        "        # Try to get a relevant row\n",
        "        # Assume first column is row names\n",
        "        row_names = df.iloc[:, 0].tolist()\n",
        "        # Find a row matching some keyword from question\n",
        "        keywords = ['revenue', 'income', 'profit', 'cost', 'assets']\n",
        "        found_row = None\n",
        "        for kw in keywords:\n",
        "            if kw in state[\"question\"].lower():\n",
        "                # find first row containing that keyword\n",
        "                for rn in row_names:\n",
        "                    if kw in str(rn).lower():\n",
        "                        found_row = rn\n",
        "                        break\n",
        "                if found_row:\n",
        "                    break\n",
        "        if found_row and year in df.columns:\n",
        "            val = df.loc[df.iloc[:, 0] == found_row, year].values[0]\n",
        "            state[\"intermediate_results\"][\"table_value\"] = val\n",
        "            state[\"intermediate_results\"][\"table_row\"] = found_row\n",
        "    return state\n",
        "\n",
        "# Node 3: Text Retrieval Agent\n",
        "# def text_retrieval_agent(state: AgentState) -> AgentState:\n",
        "#     \"\"\"Retrieve relevant paragraphs.\"\"\"\n",
        "#     question = state[\"question\"]\n",
        "#     # Debugging: Print type and attributes of retriever\n",
        "#     # print(f\"Debug: Type of retriever: {type(retriever)}\")\n",
        "#     # print(f\"Debug: Attributes of retriever: {dir(retriever)}\")\n",
        "\n",
        "#     docs = retriever.invoke(question)\n",
        "#     retrieved = [doc.page_content for doc in docs]\n",
        "#     state[\"retrieved_texts\"] = retrieved\n",
        "#     return state\n",
        "def text_retrieval_agent(state: AgentState) -> AgentState:\n",
        "    print(\"Debug: Entering text_retrieval_agent\")\n",
        "    docs = retriever.invoke(state[\"question\"])\n",
        "    state[\"retrieved_texts\"] = [doc.page_content for doc in docs]\n",
        "    # ذخیره شناسه‌ها (مثلاً order) – اگر وجود نداشت، از اندیس استفاده کنید\n",
        "    retrieved_ids = [\n",
        "        doc.metadata.get(\"order\", i) for i, doc in enumerate(docs)\n",
        "    ]\n",
        "    state[\"retrieved_paragraph_ids\"] = retrieved_ids\n",
        "    print(f\"Debug: Retrieved {len(retrieved_ids)} paragraph IDs: {retrieved_ids}\")\n",
        "    return state\n",
        "# Node 4: Calculation Agent\n",
        "def calculation_agent(state: AgentState) -> AgentState:\n",
        "    \"\"\"Perform calculations based on extracted numbers.\"\"\"\n",
        "    # This agent uses the calculator tool.\n",
        "    # For demonstration, we check if we need to compute something.\n",
        "    # We can parse the plan for an arithmetic step.\n",
        "    # Hardcoded: if question asks for \"difference\" or \"change\", we compute.\n",
        "    q_lower = state[\"question\"].lower()\n",
        "    if \"difference\" in q_lower or \"change\" in q_lower:\n",
        "        # Need two numbers. Try to get them from intermediate results.\n",
        "        vals = state[\"intermediate_results\"]\n",
        "        # In a real system, we would have extracted numbers from table and text.\n",
        "        # For demo, we simulate with dummy numbers.\n",
        "        num1 = vals.get(\"table_value\", 100)\n",
        "        num2 = vals.get(\"text_value\", 50)\n",
        "        expr = f\"{num1} - {num2}\"\n",
        "        result = calculator_tool(expr)\n",
        "        state[\"intermediate_results\"][\"calculation_result\"] = result\n",
        "    return state\n",
        "\n",
        "# Node 5: Verification Agent\n",
        "# def verification_agent(state: AgentState) -> AgentState:\n",
        "#     \"\"\"Check the consistency of the answer and produce final answer.\"\"\"\n",
        "#     # Use LLM to generate final answer based on all evidence.\n",
        "#     context = f\"Question: {state['question']}\\n\"\n",
        "#     if \"table_value\" in state[\"intermediate_results\"]:\n",
        "#         context += f\"Table value: {state['intermediate_results']['table_value']}\\n\"\n",
        "#     if \"retrieved_texts\" in state:\n",
        "#         context += f\"Retrieved texts: {' '.join(state['retrieved_texts'])}\\n\"\n",
        "#     if \"calculation_result\" in state[\"intermediate_results\"]:\n",
        "#         context += f\"Calculation result: {state['intermediate_results']['calculation_result']}\\n\"\n",
        "#     prompt = ChatPromptTemplate.from_messages([\n",
        "#         SystemMessage(content=\"You are a verification agent. Based on the evidence, provide a concise final answer to the question.\"),\n",
        "#         HumanMessage(content=context)\n",
        "#     ])\n",
        "#     response = llm.invoke(prompt.format_messages())\n",
        "#     state[\"final_answer\"] = response.content.strip()\n",
        "#     return state\n",
        "\n",
        "\n",
        "# def verification_agent(state: AgentState) -> AgentState:\n",
        "#     context = f\"Question: {state['question']}\\n\"\n",
        "#     if \"table_value\" in state[\"intermediate_results\"]:\n",
        "#         context += f\"Table value: {state['intermediate_results']['table_value']}\\n\"\n",
        "#     if \"retrieved_texts\" in state:\n",
        "#         context += f\"Retrieved texts: {' '.join(state['retrieved_texts'])}\\n\"\n",
        "#     if \"calculation_result\" in state[\"intermediate_results\"]:\n",
        "#         context += f\"Calculation result: {state['intermediate_results']['calculation_result']}\\n\"\n",
        "\n",
        "#     messages = [\n",
        "#         SystemMessage(content=\"You are a verification agent. Based on the evidence, provide the final answer concisely. Output ONLY the answer value, without any explanation or extra words.\"),\n",
        "#         HumanMessage(content=context)\n",
        "#     ]\n",
        "#     response = llm.invoke(messages)\n",
        "#     state[\"final_answer\"] = response.content.strip()\n",
        "#     return state\n",
        "\n",
        "def verification_agent(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    Produces the final answer based on all evidence.\n",
        "    Uses different prompt styles for numeric vs. descriptive questions.\n",
        "    \"\"\"\n",
        "    # Build context from all available evidence\n",
        "    context = f\"Question: {state['question']}\\n\\n\"\n",
        "\n",
        "    if \"retrieved_texts\" in state and state[\"retrieved_texts\"]:\n",
        "        context += \"Relevant paragraphs:\\n\"\n",
        "        for i, txt in enumerate(state[\"retrieved_texts\"]):\n",
        "            context += f\"{i+1}. {txt}\\n\"\n",
        "\n",
        "    if \"table_value\" in state[\"intermediate_results\"]:\n",
        "        context += f\"\\nTable value: {state['intermediate_results']['table_value']}\\n\"\n",
        "\n",
        "    if \"calculation_result\" in state[\"intermediate_results\"]:\n",
        "        context += f\"Calculation result: {state['intermediate_results']['calculation_result']}\\n\"\n",
        "\n",
        "    # Classify question type\n",
        "    q_lower = state[\"question\"].lower()\n",
        "    numeric_keywords = [\n",
        "        \"how much\", \"what is the amount\", \"gross margin\",\n",
        "        \"percentage\", \"what is the value\", \"how many\",\n",
        "        \"total\", \"sum\", \"difference\", \"change\", \"average\"\n",
        "    ]\n",
        "    is_numeric = any(kw in q_lower for kw in numeric_keywords)\n",
        "\n",
        "    if is_numeric:\n",
        "        system_msg = (\n",
        "            \"You are a verification agent for financial tables and text. \"\n",
        "            \"Based on the evidence provided, output ONLY the numeric answer \"\n",
        "            \"with its unit (%, $, etc.) if applicable. Do not add any extra words, \"\n",
        "            \"explanations, or formatting.\"\n",
        "        )\n",
        "    else:\n",
        "        system_msg = (\n",
        "            \"You are a verification agent for financial tables and text. \"\n",
        "            \"Based on the evidence provided, output the complete answer exactly \"\n",
        "            \"as stated in the evidence. Do not add any extra words or explanations.\"\n",
        "        )\n",
        "\n",
        "    messages = [\n",
        "        SystemMessage(content=system_msg),\n",
        "        HumanMessage(content=context)\n",
        "    ]\n",
        "\n",
        "    response = llm.invoke(messages)\n",
        "    answer = response.content.strip()\n",
        "\n",
        "    # Handle potential list output (if model returns a list)\n",
        "    if isinstance(answer, list):\n",
        "        answer = \" \".join(str(x) for x in answer)\n",
        "\n",
        "    state[\"final_answer\"] = answer\n",
        "    return state\n",
        "\n",
        "\n",
        "# Node 6: Error Handler\n",
        "def error_handler(state: AgentState) -> AgentState:\n",
        "    \"\"\"Handle errors gracefully.\"\"\"\n",
        "    if state.get(\"error\"):\n",
        "        state[\"final_answer\"] = f\"An error occurred: {state['error']}\"\n",
        "    return state"
      ],
      "metadata": {
        "id": "Ggq3Ighs4Obc"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Build the LangGraph\n",
        "We'll create a state graph with conditional edges."
      ],
      "metadata": {
        "id": "Qx0T6pRS9_Tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# Initialize graph\n",
        "graph = StateGraph(AgentState)\n",
        "\n",
        "# Add nodes\n",
        "graph.add_node(\"planner\", planning_agent)\n",
        "graph.add_node(\"table_parser\", table_parsing_agent)\n",
        "graph.add_node(\"text_retriever\", text_retrieval_agent)\n",
        "graph.add_node(\"calculator\", calculation_agent)\n",
        "graph.add_node(\"verifier\", verification_agent)\n",
        "graph.add_node(\"error_handler\", error_handler)\n",
        "\n",
        "# Set entry point\n",
        "graph.set_entry_point(\"planner\")\n",
        "\n",
        "# Define conditional routing from planner\n",
        "def route_from_planner(state: AgentState):\n",
        "    # If question involves table, go to table_parser, else skip.\n",
        "    if \"table\" in state[\"question\"].lower():\n",
        "        return \"table_parser\"\n",
        "    else:\n",
        "        return \"text_retriever\"\n",
        "\n",
        "graph.add_conditional_edges(\"planner\", route_from_planner, {\n",
        "    \"table_parser\": \"table_parser\",\n",
        "    \"text_retriever\": \"text_retriever\"\n",
        "})\n",
        "\n",
        "# After table_parser, go to text_retriever if needed\n",
        "graph.add_edge(\"table_parser\", \"text_retriever\")\n",
        "\n",
        "# After text_retriever, check if calculation needed\n",
        "def need_calculation(state: AgentState):\n",
        "    q = state[\"question\"].lower()\n",
        "    if any(word in q for word in [\"difference\", \"change\", \"sum\", \"total\", \"average\"]):\n",
        "        return \"calculator\"\n",
        "    else:\n",
        "        return \"verifier\"\n",
        "\n",
        "graph.add_conditional_edges(\"text_retriever\", need_calculation, {\n",
        "    \"calculator\": \"calculator\",\n",
        "    \"verifier\": \"verifier\"\n",
        "})\n",
        "\n",
        "# After calculator, go to verifier\n",
        "graph.add_edge(\"calculator\", \"verifier\")\n",
        "\n",
        "# Verifier to end\n",
        "graph.add_edge(\"verifier\", END)\n",
        "\n",
        "# Optional error handling: if any node raises exception, route to error_handler\n",
        "# For simplicity, we don't implement here.\n",
        "\n",
        "# Compile graph with memory\n",
        "memory = MemorySaver()\n",
        "app = graph.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "lr-TDFx94OYC"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Run 5 Example\n"
      ],
      "metadata": {
        "id": "k-Ih0795-MuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dev set (first 5 samples)\n",
        "dev_data = load_tatqa_split(\"tatqa_dataset_dev\")[:5]\n",
        "\n",
        "num_examples = 5\n",
        "for idx, sample in enumerate(dev_data[:num_examples]):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"EXAMPLE {idx+1}\")\n",
        "    print('='*60)\n",
        "\n",
        "    # Use the first question of each sample\n",
        "    q = sample[\"questions\"][0]\n",
        "    question = q[\"question\"]\n",
        "    gold = q[\"answer\"]\n",
        "\n",
        "    # Convert gold to string if it's a list\n",
        "    if isinstance(gold, list):\n",
        "        gold_str = \" \".join(str(x) for x in gold)\n",
        "    else:\n",
        "        gold_str = str(gold)\n",
        "\n",
        "    table_data = sample[\"table\"][\"table\"]\n",
        "    paragraphs = [p[\"text\"] for p in sample[\"paragraphs\"]]\n",
        "\n",
        "    # Display table (as DataFrame for readability)\n",
        "    df_display = pd.DataFrame(table_data[1:], columns=table_data[0])\n",
        "    print(\"Question:\", question)\n",
        "    print(\"\\nGround Truth Answer:\", gold_str)\n",
        "    print(\"\\nTable:\")\n",
        "    print(df_display.to_string())\n",
        "    print(\"\\nParagraphs:\")\n",
        "    for para in paragraphs:\n",
        "        print(f\"  - {para}\")\n",
        "\n",
        "    # Rebuild vector store with this sample's paragraphs\n",
        "    vectorstore = Chroma.from_texts(\n",
        "        texts=paragraphs,\n",
        "        embedding=embeddings,\n",
        "        metadatas=[{\"order\": i} for i in range(len(paragraphs))]\n",
        "    )\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "    # Extract headers and rows (serializable)\n",
        "    headers = table_data[0]\n",
        "    rows = table_data[1:]\n",
        "\n",
        "    # Prepare initial state\n",
        "    initial_state = AgentState(\n",
        "        question=question,\n",
        "        table_headers=headers,\n",
        "        table_rows=rows,\n",
        "        paragraphs=paragraphs,\n",
        "        retrieved_texts=[],\n",
        "        extracted_numbers=[],\n",
        "        plan=[],\n",
        "        intermediate_results={},\n",
        "        final_answer=None,\n",
        "        error=None\n",
        "    )\n",
        "\n",
        "    # Run the graph with a unique thread_id\n",
        "    result = app.invoke(initial_state, config={\"configurable\": {\"thread_id\": f\"dev_{idx}\"}})\n",
        "    pred = result[\"final_answer\"]\n",
        "\n",
        "    # Convert prediction to string if it's a list\n",
        "    if isinstance(pred, list):\n",
        "        pred = \" \".join(str(x) for x in pred)\n",
        "    elif pred is None:\n",
        "        pred = \"\"\n",
        "\n",
        "    print(\"\\nModel Answer:\", pred)\n",
        "    print('='*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8Bm8N3vqZIB",
        "outputId": "61d102d3-f68d-4da6-b9f1-f64fafd80a5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "EXAMPLE 1\n",
            "============================================================\n",
            "Question: What is the gross margin for F19?\n",
            "\n",
            "Ground Truth Answer: 22.9%\n",
            "\n",
            "Table:\n",
            "                                        F19   F18 (3)                 CHANGE\n",
            "0                       $ MILLION  53 WEEKS  52 WEEKS     CHANGE  NORMALISED\n",
            "1                           Sales     8,657     8,244       5.0%        3.2%\n",
            "2                          EBITDA       579       603     (4.1)%      (5.4)%\n",
            "3   Depreciation and amortisation     (105)      (87)      20.1%       20.1%\n",
            "4                            EBIT       474       516     (8.2)%      (9.7)%\n",
            "5                Gross margin (%)      22.9      23.1   (16) bps    (14) bps\n",
            "6      Cost of doing business (%)      17.4      16.8     63 bps      64 bps\n",
            "7               EBIT to sales (%)       5.5       6.3   (78) bps    (78) bps\n",
            "8    Sales per square metre ($)$)    18,675    18,094       3.2%        1.4%\n",
            "9                  Funds employed     3,185     3,214     (0.9)%            \n",
            "10                       ROFE (%)      15.2      17.1  (190) bps   (215) bps\n",
            "\n",
            "Paragraphs:\n",
            "  - In Endeavour Drinks, BWS and Dan Murphy’s key VOC metrics ended F19 at record highs, with improvements both in store and Online. Sales increased by 5.0% (3.2% normalised) to $8.7 billion with comparable sales increasing 2.3%. The market remained subdued throughout the year with declining volumes offset by price and mix improvements. Sales growth in H2 improved on H1 in both Dan Murphy’s and BWS, with Endeavour Drinks’ sales increasing by 4.8% (normalised) with comparable sales increasing 4.0%, compared to 0.7% growth in H1. The timing of New Year’s Day boosted sales in H2 by 84 bps and Q3, in particular, also benefitted from more stable weather compared to Q2. Dan Murphy’s focus on ‘discovery’ driven range, service and convenience is also beginning to resonate with customers.\n",
            "  - BWS maintained its strong trading momentum, with enhancements to localised ranging and tailored Woolworths Rewards offerings. The BWS store network grew to 1,346 stores with 30 net new stores and the new BWS Renewal format successfully extended to key urban standalone stores. BWS’ convenience offering continued to expand, with On Demand delivery now available in 605 stores, supporting double digit online sales growth. Jimmy Brings expanded its geographical reach to Brisbane, Gold Coast, Canberra and new suburbs in Sydney and Melbourne.\n",
            "  - Dan Murphy’s delivered double digit Online sales growth with new customer offerings, including the roll out of On Demand delivery to 91 stores and 30‐minute Pick up from all stores. In store customer experience was enhanced with the introduction of wine merchants in key stores, to improve team product knowledge and customer discovery, while memberships in My Dan’s loyalty program increased 15% on the prior year. Dan Murphy’s store network grew to 230 with three new store openings in Q4 including the first store to be powered by solar energy.\n",
            "  - Endeavour Drinks sales per square metre increased by 3.2% (1.4% normalised) with sales growth above net average space growth of 1.7%.\n",
            "  - Gross margin was 22.9%, 14 bps down on a normalised basis, with trading margin improvements offset by higher freight costs attributable to petrol prices, growth in online delivery and category mix.\n",
            "  - Normalised CODB as a percentage of sales grew 64 bps, driven by a $21 million impairment charge related to goodwill and other intangible assets associated with the Summergate business in China. Summergate has now transitioned to ExportCo. Excluding Summergate, normalised CODB as a percentage of sales increased by 40 bps due to above inflationary cost pressures, as well as targeted investment in key focus areas including customer experience, ranging, data and analytics.\n",
            "  - Endeavour Drinks EBIT for F19 decreased 8.2% to $474 million. EBIT normalised for the 53rd week and Summergate impairment of $21 million decreased 5.6%. Normalised ROFE (excluding the Summergate impairment) declined 148 bps driven by the decline in EBIT.\n",
            "  - (3) During the period, the management of the New Zealand Wine Cellars business transferred from Endeavour Drinks to New Zealand Food. The prior period has been re‑presented toconform with the current period presentation.\n",
            "Debug: Type of retriever: <class 'langchain_core.vectorstores.base.VectorStoreRetriever'>\n",
            "Debug: Attributes of retriever: ['InputType', 'OutputType', '__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__orig_bases__', '__parameters__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_on_complete__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_setattr_handlers__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__ror__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abatch_with_config', '_abc_impl', '_acall_with_config', '_aget_relevant_documents', '_atransform_stream_with_config', '_batch_with_config', '_calculate_keys', '_call_with_config', '_copy_and_set_values', '_expects_other_args', '_get_ls_params', '_get_relevant_documents', '_get_value', '_iter', '_new_arg_supported', '_setattr_handler', '_transform_stream_with_config', 'aadd_documents', 'abatch', 'abatch_as_completed', 'add_documents', 'ainvoke', 'allowed_search_types', 'as_tool', 'assign', 'astream', 'astream_events', 'astream_log', 'atransform', 'batch', 'batch_as_completed', 'bind', 'config_schema', 'config_specs', 'configurable_alternatives', 'configurable_fields', 'construct', 'copy', 'dict', 'from_orm', 'get_config_jsonschema', 'get_graph', 'get_input_jsonschema', 'get_input_schema', 'get_lc_namespace', 'get_name', 'get_output_jsonschema', 'get_output_schema', 'get_prompts', 'input_schema', 'invoke', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'map', 'metadata', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'name', 'output_schema', 'parse_file', 'parse_obj', 'parse_raw', 'pick', 'pipe', 'schema', 'schema_json', 'search_kwargs', 'search_type', 'stream', 'tags', 'to_json', 'to_json_not_implemented', 'transform', 'update_forward_refs', 'validate', 'validate_search_type', 'vectorstore', 'with_alisteners', 'with_config', 'with_fallbacks', 'with_listeners', 'with_retry', 'with_types']\n",
            "\n",
            "Model Answer: 22.9%\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "EXAMPLE 2\n",
            "============================================================\n",
            "Question: What is the company paid on a cost-plus type contract?\n",
            "\n",
            "Ground Truth Answer: our allowable incurred costs plus a profit which can be fixed or variable depending on the contract’s fee arrangement up to predetermined funding levels determined by the customer\n",
            "\n",
            "Table:\n",
            "                           Years Ended September 30,            \n",
            "0                     2019                      2018        2017\n",
            "1  Fixed Price  $  1,452.4                $  1,146.2  $  1,036.9\n",
            "2        Other        44.1                      56.7        70.8\n",
            "3  Total sales    $1,496.5                  $1,202.9    $1,107.7\n",
            "\n",
            "Paragraphs:\n",
            "  - Sales by Contract Type: Substantially all of our contracts are fixed-price type contracts. Sales included in Other contract types represent cost plus and time and material type contracts.\n",
            "  - On a fixed-price type contract, we agree to perform the contractual statement of work for a predetermined sales price. On a cost-plus type contract, we are paid our allowable incurred costs plus a profit which can be fixed or variable depending on the contract’s fee arrangement up to predetermined funding levels determined by the customer. On a time-and-material type contract, we are paid on the basis of direct labor hours expended at specified fixed-price hourly rates (that include wages, overhead, allowable general and administrative expenses and profit) and materials at cost. The table below presents total net sales disaggregated by contract type (in millions):\n",
            "Debug: Type of retriever: <class 'langchain_core.vectorstores.base.VectorStoreRetriever'>\n",
            "Debug: Attributes of retriever: ['InputType', 'OutputType', '__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__orig_bases__', '__parameters__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_on_complete__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_setattr_handlers__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__ror__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abatch_with_config', '_abc_impl', '_acall_with_config', '_aget_relevant_documents', '_atransform_stream_with_config', '_batch_with_config', '_calculate_keys', '_call_with_config', '_copy_and_set_values', '_expects_other_args', '_get_ls_params', '_get_relevant_documents', '_get_value', '_iter', '_new_arg_supported', '_setattr_handler', '_transform_stream_with_config', 'aadd_documents', 'abatch', 'abatch_as_completed', 'add_documents', 'ainvoke', 'allowed_search_types', 'as_tool', 'assign', 'astream', 'astream_events', 'astream_log', 'atransform', 'batch', 'batch_as_completed', 'bind', 'config_schema', 'config_specs', 'configurable_alternatives', 'configurable_fields', 'construct', 'copy', 'dict', 'from_orm', 'get_config_jsonschema', 'get_graph', 'get_input_jsonschema', 'get_input_schema', 'get_lc_namespace', 'get_name', 'get_output_jsonschema', 'get_output_schema', 'get_prompts', 'input_schema', 'invoke', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'map', 'metadata', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'name', 'output_schema', 'parse_file', 'parse_obj', 'parse_raw', 'pick', 'pipe', 'schema', 'schema_json', 'search_kwargs', 'search_type', 'stream', 'tags', 'to_json', 'to_json_not_implemented', 'transform', 'update_forward_refs', 'validate', 'validate_search_type', 'vectorstore', 'with_alisteners', 'with_config', 'with_fallbacks', 'with_listeners', 'with_retry', 'with_types']\n",
            "\n",
            "Model Answer: On a cost-plus type contract, we are paid our allowable incurred costs plus a profit which can be fixed or variable depending on the contract’s fee arrangement up to predetermined funding levels determined by the customer.\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "EXAMPLE 3\n",
            "============================================================\n",
            "Question: How is industry end market information presented?\n",
            "\n",
            "Ground Truth Answer: consistently with our internal management reporting and may be revised periodically as management deems necessary.\n",
            "\n",
            "Table:\n",
            "                                                       Fiscal          \n",
            "0                                         2019           2018      2017\n",
            "1                                               (in millions)          \n",
            "2          Transportation Solutions:                                   \n",
            "3                         Automotive   $ 5,686        $ 6,092  $  5,228\n",
            "4          Commercial transportation     1,221          1,280       997\n",
            "5                            Sensors       914            918       814\n",
            "6     Total Transportation Solutions     7,821          8,290     7,039\n",
            "7              Industrial Solutions:                                   \n",
            "8               Industrial equipment     1,949          1,987     1,747\n",
            "9   Aerospace, defense, oil, and gas     1,306          1,157     1,075\n",
            "10                            Energy       699            712       685\n",
            "11        Total Industrial Solutions     3,954          3,856     3,507\n",
            "12         Communications Solutions:                                   \n",
            "13                  Data and devices       993          1,068       963\n",
            "14                        Appliances       680            774       676\n",
            "15    Total Communications Solutions     1,673          1,842     1,639\n",
            "16                             Total  $ 13,448       $ 13,988  $ 12,185\n",
            "\n",
            "Paragraphs:\n",
            "  - Net sales by segment and industry end market(1) were as follows:\n",
            "  - (1) Industry end market information is presented consistently with our internal management reporting and may be revised periodically as management deems necessary.\n",
            "Debug: Type of retriever: <class 'langchain_core.vectorstores.base.VectorStoreRetriever'>\n",
            "Debug: Attributes of retriever: ['InputType', 'OutputType', '__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__orig_bases__', '__parameters__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_on_complete__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_setattr_handlers__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__ror__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abatch_with_config', '_abc_impl', '_acall_with_config', '_aget_relevant_documents', '_atransform_stream_with_config', '_batch_with_config', '_calculate_keys', '_call_with_config', '_copy_and_set_values', '_expects_other_args', '_get_ls_params', '_get_relevant_documents', '_get_value', '_iter', '_new_arg_supported', '_setattr_handler', '_transform_stream_with_config', 'aadd_documents', 'abatch', 'abatch_as_completed', 'add_documents', 'ainvoke', 'allowed_search_types', 'as_tool', 'assign', 'astream', 'astream_events', 'astream_log', 'atransform', 'batch', 'batch_as_completed', 'bind', 'config_schema', 'config_specs', 'configurable_alternatives', 'configurable_fields', 'construct', 'copy', 'dict', 'from_orm', 'get_config_jsonschema', 'get_graph', 'get_input_jsonschema', 'get_input_schema', 'get_lc_namespace', 'get_name', 'get_output_jsonschema', 'get_output_schema', 'get_prompts', 'input_schema', 'invoke', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'map', 'metadata', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'name', 'output_schema', 'parse_file', 'parse_obj', 'parse_raw', 'pick', 'pipe', 'schema', 'schema_json', 'search_kwargs', 'search_type', 'stream', 'tags', 'to_json', 'to_json_not_implemented', 'transform', 'update_forward_refs', 'validate', 'validate_search_type', 'vectorstore', 'with_alisteners', 'with_config', 'with_fallbacks', 'with_listeners', 'with_retry', 'with_types']\n",
            "\n",
            "Model Answer: Industry end market information is presented consistently with our internal management reporting and may be revised periodically as management deems necessary.\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "EXAMPLE 4\n",
            "============================================================\n",
            "Question: How is the discount rate for domestic plans determined?\n",
            "\n",
            "Ground Truth Answer: By comparison against the FTSE pension liability index for AA rated corporate instruments\n",
            "\n",
            "Table:\n",
            "                                        Domestic         International       \n",
            "0                                  September 30,         September 30,       \n",
            "1                                           2019   2018           2019   2018\n",
            "2                   Discount rate          4.00%  3.75%          1.90%  2.80%\n",
            "3  Expected return on plan assets                                3.40%  3.70%\n",
            "4   Rate of compensation increase                                - - %  - - %\n",
            "\n",
            "Paragraphs:\n",
            "  - The following table provides the weighted average actuarial assumptions used to determine net periodic benefit costfor years ended:\n",
            "  - For domestic plans, the discount rate was determined by comparison against the FTSE pension liability index for AA rated corporate instruments. The Company monitors other indices to assure that the pension obligations are fairly reported on a consistent basis. The international discount rates were determined by comparison against country specific AA corporate indices, adjusted for duration of the obligation.\n",
            "  - The periodic benefit cost and the actuarial present value of projected benefit obligations are based on actuarial assumptions that are reviewed on an annual basis. The Company revises these assumptions based on an annual evaluation of longterm trends, as well as market conditions that may have an impact on the cost of providing retirement benefits.\n",
            "Debug: Type of retriever: <class 'langchain_core.vectorstores.base.VectorStoreRetriever'>\n",
            "Debug: Attributes of retriever: ['InputType', 'OutputType', '__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__orig_bases__', '__parameters__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_on_complete__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_setattr_handlers__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__ror__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abatch_with_config', '_abc_impl', '_acall_with_config', '_aget_relevant_documents', '_atransform_stream_with_config', '_batch_with_config', '_calculate_keys', '_call_with_config', '_copy_and_set_values', '_expects_other_args', '_get_ls_params', '_get_relevant_documents', '_get_value', '_iter', '_new_arg_supported', '_setattr_handler', '_transform_stream_with_config', 'aadd_documents', 'abatch', 'abatch_as_completed', 'add_documents', 'ainvoke', 'allowed_search_types', 'as_tool', 'assign', 'astream', 'astream_events', 'astream_log', 'atransform', 'batch', 'batch_as_completed', 'bind', 'config_schema', 'config_specs', 'configurable_alternatives', 'configurable_fields', 'construct', 'copy', 'dict', 'from_orm', 'get_config_jsonschema', 'get_graph', 'get_input_jsonschema', 'get_input_schema', 'get_lc_namespace', 'get_name', 'get_output_jsonschema', 'get_output_schema', 'get_prompts', 'input_schema', 'invoke', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'map', 'metadata', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'name', 'output_schema', 'parse_file', 'parse_obj', 'parse_raw', 'pick', 'pipe', 'schema', 'schema_json', 'search_kwargs', 'search_type', 'stream', 'tags', 'to_json', 'to_json_not_implemented', 'transform', 'update_forward_refs', 'validate', 'validate_search_type', 'vectorstore', 'with_alisteners', 'with_config', 'with_fallbacks', 'with_listeners', 'with_retry', 'with_types']\n",
            "\n",
            "Model Answer: For domestic plans, the discount rate was determined by comparison against the FTSE pension liability index for AA rated corporate instruments. The Company monitors other indices to assure that the pension obligations are fairly reported on a consistent basis. The international discount rates were determined by comparison against country specific AA corporate indices, adjusted for duration of the obligation.\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "EXAMPLE 5\n",
            "============================================================\n",
            "Question: What financial items are listed in the table?\n",
            "\n",
            "Ground Truth Answer: Defined contribution schemes Defined benefit schemes \n",
            "\n",
            "Table:\n",
            "                             Income statement expense                           \n",
            "0                                                      2019 €m  2018 €m  2017 €m\n",
            "1                        Defined contribution schemes      166      178      192\n",
            "2                             Defined benefit schemes       57       44       20\n",
            "3  Total amount charged to income statement (note 23)      223      222      212\n",
            "\n",
            "Paragraphs:\n",
            "  - 24. Post employment benefits\n",
            "  - The Group operates a number of defined benefit and defined contribution pension plans for our employees. The Group’s largest defined benefit scheme is in the UK. For further details see “Critical accounting judgements and key sources of estimation uncertainty” in note 1 to the consolidated financial statements.\n",
            "  - Accounting policies\n",
            "  - For defined benefit retirement plans, the difference between the fair value of the plan assets and the present value of the plan liabilities is recognised as an asset or liability on the statement of financial position. Scheme liabilities are assessed using the projected unit funding method and applying the principal actuarial assumptions at the reporting period date. Assets are valued at market value.\n",
            "  - Actuarial gains and losses are taken to the statement of comprehensive income as incurred. For this purpose, actuarial gains and losses comprise both the effects of changes in actuarial assumptions and experience adjustments arising from differences between the previous actuarial assumptions and what has actually occurred. The return on plan assets, in excess of interest income, and costs incurred for the management of plan assets are also taken to other comprehensive income.\n",
            "  - Other movements in the net surplus or deficit are recognised in the income statement, including the current service cost, any past service cost and the effect of any settlements. The interest cost less the expected interest income on assets is also charged to the income statement. The amount charged to the income statement in respect of these plans is included within operating costs or in the Group’s share of the results of equity accounted operations, as appropriate\n",
            "  - The Group’s contributions to defined contribution pension plans are charged to the income statement as they fall due.\n",
            "  - Background\n",
            "  - At 31 March 2019 the Group operated a number of pension plans for the benefit of its employees throughout the world, with varying rights and obligations depending on the conditions and practices in the countries concerned. The Group’s pension plans are provided through both defined benefit and defined contribution arrangements. Defined benefit schemes provide benefits based on the employees’ length of pensionable service and their final pensionable salary or other criteria. Defined contribution schemes offer employees individual funds that are converted into benefits at the time of retirement\n",
            "  - The Group operates defined benefit schemes in Germany, Ghana, India, Ireland, Italy, the UK, the United States and the Group operates defined benefit indemnity plans in Greece and Turkey. Defined contribution pension schemes are currently provided in Egypt, Germany, Greece, Hungary, India, Ireland, Italy, New Zealand, Portugal, South Africa, Spain and the UK.\n",
            "  - Defined benefit schemes\n",
            "  - The Group’s retirement policy is to provide competitive pension provision, in each operating country, in line with the market median for that location. The Group’s preferred retirement provision is focused on Defined Contribution (‘DC’) arrangements and/or State provision for future service.\n",
            "  - The Group’s main defined benefit funding liability is the Vodafone UK Group Pension Scheme (‘Vodafone UK plan’). Since June 2014 the plan has consisted of two segregated sections: the Vodafone Section and the Cable & Wireless Section (‘CWW Section’). Both sections are closed to new entrants and to future accrual. The Group also operates smaller funded and unfunded plans in the UK, funded and unfunded plans in Germany and funded plans in Ireland. Defined benefit pension provision exposes the Group to actuarial risks such as longer than expected longevity of participants, lower than expected return on investments and higher than expected inflation, which may increase the liabilities or reduce the value of assets of the schemes.\n",
            "  - The main defined benefit schemes are administered by trustee boards which are legally separate from the Group and consist of representatives who are employees, former employees or are independent from the Company. The boards of the pension schemes are required by legislation to act in the best interest of the participants, set the investment strategy and contribution rates and are subject to statutory funding objectives.\n",
            "  - The Vodafone UK plan is registered as an occupational pension plan with HM Revenue and Customs (‘HMRC’) and is subject to UK legislation and operates within the framework outlined by the Pensions Regulator. UK legislation requires that pension schemes are funded prudently and that valuations are undertaken at least every three years. Separate valuations are required for the Vodafone Section and CWW Section.\n",
            "  - The trustees obtain regular actuarial valuations to check whether the statutory funding objective is met and whether a recovery plan is required to restore funding to the level of the agreed technical provisions. On 19 October 2017, the 31 March 2016 triennial actuarial valuation for the Vodafone Section and CWW Section of the Vodafone UK plan, which is used to judge the funding the Group needs to put into the scheme, was concluded.\n",
            "Debug: Type of retriever: <class 'langchain_core.vectorstores.base.VectorStoreRetriever'>\n",
            "Debug: Attributes of retriever: ['InputType', 'OutputType', '__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__orig_bases__', '__parameters__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_on_complete__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_setattr_handlers__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__ror__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abatch_with_config', '_abc_impl', '_acall_with_config', '_aget_relevant_documents', '_atransform_stream_with_config', '_batch_with_config', '_calculate_keys', '_call_with_config', '_copy_and_set_values', '_expects_other_args', '_get_ls_params', '_get_relevant_documents', '_get_value', '_iter', '_new_arg_supported', '_setattr_handler', '_transform_stream_with_config', 'aadd_documents', 'abatch', 'abatch_as_completed', 'add_documents', 'ainvoke', 'allowed_search_types', 'as_tool', 'assign', 'astream', 'astream_events', 'astream_log', 'atransform', 'batch', 'batch_as_completed', 'bind', 'config_schema', 'config_specs', 'configurable_alternatives', 'configurable_fields', 'construct', 'copy', 'dict', 'from_orm', 'get_config_jsonschema', 'get_graph', 'get_input_jsonschema', 'get_input_schema', 'get_lc_namespace', 'get_name', 'get_output_jsonschema', 'get_output_schema', 'get_prompts', 'input_schema', 'invoke', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'map', 'metadata', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'name', 'output_schema', 'parse_file', 'parse_obj', 'parse_raw', 'pick', 'pipe', 'schema', 'schema_json', 'search_kwargs', 'search_type', 'stream', 'tags', 'to_json', 'to_json_not_implemented', 'transform', 'update_forward_refs', 'validate', 'validate_search_type', 'vectorstore', 'with_alisteners', 'with_config', 'with_fallbacks', 'with_listeners', 'with_retry', 'with_types']\n",
            "\n",
            "Model Answer: The evidence does not provide any specific financial items listed in the table.\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select first 5 samples from test set\n",
        "num_examples = 5\n",
        "for idx, sample in enumerate(test_data[:num_examples]):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"EXAMPLE {idx+1}\")\n",
        "    print('='*60)\n",
        "\n",
        "    question = sample[\"questions\"][0][\"question\"]\n",
        "    table_data = sample[\"table\"][\"table\"]\n",
        "    paragraphs = [p[\"text\"] for p in sample[\"paragraphs\"]]\n",
        "\n",
        "    # Convert table to DataFrame just for display\n",
        "    df_display = pd.DataFrame(table_data[1:], columns=table_data[0])\n",
        "    print(\"Question:\", question)\n",
        "    print(\"\\nTable:\")\n",
        "    print(df_display.to_string())\n",
        "    print(\"\\nParagraphs:\")\n",
        "    for para in paragraphs:\n",
        "        print(f\"  - {para}\")\n",
        "\n",
        "    # Rebuild vector store with this sample's paragraphs\n",
        "    vectorstore = Chroma.from_texts(\n",
        "        texts=paragraphs,\n",
        "        embedding=embeddings,\n",
        "        metadatas=[{\"order\": i} for i in range(len(paragraphs))]\n",
        "    )\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "    # Extract headers and rows (serializable)\n",
        "    headers = table_data[0]\n",
        "    rows = table_data[1:]\n",
        "\n",
        "    # Prepare initial state\n",
        "    initial_state = AgentState(\n",
        "        question=question,\n",
        "        table_headers=headers,\n",
        "        table_rows=rows,\n",
        "        paragraphs=paragraphs,\n",
        "        retrieved_texts=[],\n",
        "        extracted_numbers=[],\n",
        "        plan=[],\n",
        "        intermediate_results={},\n",
        "        final_answer=None,\n",
        "        error=None\n",
        "    )\n",
        "\n",
        "    # Run the graph (use a unique thread_id per example)\n",
        "    result = app.invoke(initial_state, config={\"configurable\": {\"thread_id\": f\"test_{idx}\"}})\n",
        "    print(\"\\nFinal Answer:\", result[\"final_answer\"])\n",
        "    print('='*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXIL48Ii4OUM",
        "outputId": "7dcbbc88-1e84-4724-ed9b-6907099a310a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "EXAMPLE 1\n",
            "============================================================\n",
            "Question: What was the amount of unrecognized stock-based compensation expense related to unvested employee stock options in 2019?\n",
            "\n",
            "Table:\n",
            "                                                     Year Ended         Year Ended\n",
            "0  Stock-Based Compensation by Type of Award  December 31, 2019  December 31, 2018\n",
            "1                              Stock options             $2,756             $2,926\n",
            "2                                       RSUs                955              1,129\n",
            "3     Total stock-based compensation expense             $3,711             $4,055\n",
            "\n",
            "Paragraphs:\n",
            "  - Stock-based compensation expense is included in general and administrative expense for each period as follows:\n",
            "  - As of December 31, 2019, there was $4,801 of unrecognized stock-based compensation expense related to unvested employee stock options and $1,882 of unrecognized stock-based compensation expense related to unvested RSUs. These costs are expected to be recognized over a weighted-average period of 2.13 and 2.33 years, respectively.\n",
            "Debug: Type of retriever: <class 'langchain_core.vectorstores.base.VectorStoreRetriever'>\n",
            "Debug: Attributes of retriever: ['InputType', 'OutputType', '__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__orig_bases__', '__parameters__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_on_complete__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_setattr_handlers__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__ror__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abatch_with_config', '_abc_impl', '_acall_with_config', '_aget_relevant_documents', '_atransform_stream_with_config', '_batch_with_config', '_calculate_keys', '_call_with_config', '_copy_and_set_values', '_expects_other_args', '_get_ls_params', '_get_relevant_documents', '_get_value', '_iter', '_new_arg_supported', '_setattr_handler', '_transform_stream_with_config', 'aadd_documents', 'abatch', 'abatch_as_completed', 'add_documents', 'ainvoke', 'allowed_search_types', 'as_tool', 'assign', 'astream', 'astream_events', 'astream_log', 'atransform', 'batch', 'batch_as_completed', 'bind', 'config_schema', 'config_specs', 'configurable_alternatives', 'configurable_fields', 'construct', 'copy', 'dict', 'from_orm', 'get_config_jsonschema', 'get_graph', 'get_input_jsonschema', 'get_input_schema', 'get_lc_namespace', 'get_name', 'get_output_jsonschema', 'get_output_schema', 'get_prompts', 'input_schema', 'invoke', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'map', 'metadata', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'name', 'output_schema', 'parse_file', 'parse_obj', 'parse_raw', 'pick', 'pipe', 'schema', 'schema_json', 'search_kwargs', 'search_type', 'stream', 'tags', 'to_json', 'to_json_not_implemented', 'transform', 'update_forward_refs', 'validate', 'validate_search_type', 'vectorstore', 'with_alisteners', 'with_config', 'with_fallbacks', 'with_listeners', 'with_retry', 'with_types']\n",
            "\n",
            "Final Answer: The amount of unrecognized stock-based compensation expense related to unvested employee stock options in 2019 was $4,801.\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "EXAMPLE 2\n",
            "============================================================\n",
            "Question: What is the Share Incentive Plan ('SIP')?\n",
            "\n",
            "Table:\n",
            "                                                    Group       Company      \n",
            "0                                                    2019  2018    2019  2018\n",
            "1                                                      £m    £m      £m    £m\n",
            "2                      Share Incentive Plan (‘SIP’)     –   0.8       –     –\n",
            "3                         Sharesave scheme (‘SAYE’)   0.3   0.3       –     –\n",
            "4                    Performance Share Plan (‘PSP’)   2.1   1.8     1.3   0.7\n",
            "5   Deferred Annual Bonus and Single Incentive Plan   2.3   0.4     0.4   0.2\n",
            "6                  Total share-based payment charge   4.7   3.3     1.7   0.9\n",
            "7  NI and apprenticeship levy on applicable schemes   1.2   0.4     0.6   0.1\n",
            "8                                      Total charge   5.9   3.7     2.3   1.0\n",
            "\n",
            "Paragraphs:\n",
            "  - 29. Share-based payments continued\n",
            "  - Share Incentive Plan\n",
            "  - In 2015, the Group established a Share Incentive Plan (‘SIP’). All eligible employees were awarded free shares (or nil-cost options in the case of employees in Ireland) valued at £3,600 each based on the share price at the time of the Company’s admission to the Stock Exchange in March 2015, subject to a three-year service period (‘Vesting Period’). The SIP shareholders are entitled to dividends over the Vesting Period. There are no performance conditions applicable to the vesting of SIP shares. The fair value of the SIP awards at the grant date was measured to be £2.72 using the Black-Scholes model. The resulting share-based payments charge is being spread evenly over the Vesting Period.\n",
            "Debug: Type of retriever: <class 'langchain_core.vectorstores.base.VectorStoreRetriever'>\n",
            "Debug: Attributes of retriever: ['InputType', 'OutputType', '__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__orig_bases__', '__parameters__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_on_complete__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_setattr_handlers__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__ror__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abatch_with_config', '_abc_impl', '_acall_with_config', '_aget_relevant_documents', '_atransform_stream_with_config', '_batch_with_config', '_calculate_keys', '_call_with_config', '_copy_and_set_values', '_expects_other_args', '_get_ls_params', '_get_relevant_documents', '_get_value', '_iter', '_new_arg_supported', '_setattr_handler', '_transform_stream_with_config', 'aadd_documents', 'abatch', 'abatch_as_completed', 'add_documents', 'ainvoke', 'allowed_search_types', 'as_tool', 'assign', 'astream', 'astream_events', 'astream_log', 'atransform', 'batch', 'batch_as_completed', 'bind', 'config_schema', 'config_specs', 'configurable_alternatives', 'configurable_fields', 'construct', 'copy', 'dict', 'from_orm', 'get_config_jsonschema', 'get_graph', 'get_input_jsonschema', 'get_input_schema', 'get_lc_namespace', 'get_name', 'get_output_jsonschema', 'get_output_schema', 'get_prompts', 'input_schema', 'invoke', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'map', 'metadata', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'name', 'output_schema', 'parse_file', 'parse_obj', 'parse_raw', 'pick', 'pipe', 'schema', 'schema_json', 'search_kwargs', 'search_type', 'stream', 'tags', 'to_json', 'to_json_not_implemented', 'transform', 'update_forward_refs', 'validate', 'validate_search_type', 'vectorstore', 'with_alisteners', 'with_config', 'with_fallbacks', 'with_listeners', 'with_retry', 'with_types']\n",
            "\n",
            "Final Answer: The Share Incentive Plan (SIP) is a program established in 2015 that awards eligible employees free shares (or nil-cost options for employees in Ireland) valued at £3,600 each, based on the share price at the time of the company's admission to the Stock Exchange. Employees must complete a three-year service period (Vesting Period) to receive the shares, which also entitle them to dividends during this period. There are no performance conditions for the vesting of SIP shares, and the fair value of the awards at grant date was measured at £2.72 using the Black-Scholes model, with the share-based payments charge spread evenly over the Vesting Period.\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "dc6c2140",
        "outputId": "ed9a241b-df11-45ca-c381-662e5dc08b83"
      },
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    # Generate and display the graph visualization\n",
        "    display(Image(app.get_graph().draw_mermaid_png()))\n",
        "except Exception as e:\n",
        "    print(f\"Could not generate graph visualization: {e}\")\n",
        "    # Fallback to printing the nodes and edges if image generation fails\n",
        "    print(\"\\nNodes:\", graph.nodes.keys())\n",
        "    print(\"Edges:\", graph.edges)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Evaluate on a Subset of Test Set"
      ],
      "metadata": {
        "id": "Vmatr2lzOKlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_answer(s: Any) -> str:\n",
        "    \"\"\"Lower text and remove punctuation, articles, extra whitespace. Handles lists and non-strings safely.\"\"\"\n",
        "    import re\n",
        "    if isinstance(s, list):\n",
        "        s = \" \".join(str(x) for x in s)\n",
        "    if s is None:\n",
        "        return \"\"\n",
        "\n",
        "    # Ensure it's a string before calling string methods\n",
        "    s_str = str(s).lower().strip()\n",
        "    s_str = re.sub(r'[^\\w\\s]', '', s_str)\n",
        "    s_str = re.sub(r'\\b(a|an|the)\\b', '', s_str)\n",
        "    s_str = re.sub(r'\\s+', ' ', s_str).strip()\n",
        "    return s_str\n",
        "\n",
        "def compute_em(pred, gold):\n",
        "    return int(normalize_answer(pred) == normalize_answer(gold))\n",
        "\n",
        "def compute_f1(pred, gold):\n",
        "    pred_tokens = normalize_answer(pred).split()\n",
        "    gold_tokens = normalize_answer(gold).split()\n",
        "    common = set(pred_tokens) & set(gold_tokens)\n",
        "    if not common:\n",
        "        return 0.0\n",
        "    prec = len(common) / len(pred_tokens)\n",
        "    rec = len(common) / len(gold_tokens)\n",
        "    return 2 * prec * rec / (prec + rec)\n",
        "\n",
        "# Load dev set\n",
        "dev_data = load_tatqa_split(\"tatqa_dataset_dev\")[:5]\n",
        "results = []\n",
        "for i, sample in enumerate(dev_data):\n",
        "    for q in sample[\"questions\"][:2]:\n",
        "        question = q[\"question\"]\n",
        "        gold = q[\"answer\"]\n",
        "\n",
        "        table_data = sample[\"table\"][\"table\"]\n",
        "        headers = table_data[0]\n",
        "        rows = table_data[1:]\n",
        "        paragraphs = [p[\"text\"] for p in sample[\"paragraphs\"]]\n",
        "\n",
        "        state = AgentState(\n",
        "            question=question,\n",
        "            table_headers=headers,\n",
        "            table_rows=rows,\n",
        "            paragraphs=paragraphs,\n",
        "            retrieved_texts=[],\n",
        "            extracted_numbers=[],\n",
        "            plan=[],\n",
        "            intermediate_results={},\n",
        "            final_answer=None,\n",
        "            error=None\n",
        "        )\n",
        "\n",
        "        out = app.invoke(state, config={\"configurable\": {\"thread_id\": f\"eval{i}\"}})\n",
        "        pred = out[\"final_answer\"]\n",
        "\n",
        "        em = compute_em(pred, gold)\n",
        "        f1 = compute_f1(pred, gold)\n",
        "        results.append({\n",
        "            \"question\": question,\n",
        "            \"gold\": gold,\n",
        "            \"pred\": pred,\n",
        "            \"em\": em,\n",
        "            \"f1\": f1\n",
        "        })\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "display(df_results[[\"question\", \"gold\", \"pred\", \"em\", \"f1\"]])\n",
        "print(f\"Average EM: {df_results['em'].mean():.2f}\")\n",
        "print(f\"Average F1: {df_results['f1'].mean():.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "id": "UX5HV0B44OQO",
        "outputId": "17b15acb-326a-4d89-f1ea-a2295ab85166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: Type of retriever: <class 'langchain_core.vectorstores.base.VectorStoreRetriever'>\n",
            "Debug: Attributes of retriever: ['InputType', 'OutputType', '__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__orig_bases__', '__parameters__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_on_complete__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_setattr_handlers__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__ror__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abatch_with_config', '_abc_impl', '_acall_with_config', '_aget_relevant_documents', '_atransform_stream_with_config', '_batch_with_config', '_calculate_keys', '_call_with_config', '_copy_and_set_values', '_expects_other_args', '_get_ls_params', '_get_relevant_documents', '_get_value', '_iter', '_new_arg_supported', '_setattr_handler', '_transform_stream_with_config', 'aadd_documents', 'abatch', 'abatch_as_completed', 'add_documents', 'ainvoke', 'allowed_search_types', 'as_tool', 'assign', 'astream', 'astream_events', 'astream_log', 'atransform', 'batch', 'batch_as_completed', 'bind', 'config_schema', 'config_specs', 'configurable_alternatives', 'configurable_fields', 'construct', 'copy', 'dict', 'from_orm', 'get_config_jsonschema', 'get_graph', 'get_input_jsonschema', 'get_input_schema', 'get_lc_namespace', 'get_name', 'get_output_jsonschema', 'get_output_schema', 'get_prompts', 'input_schema', 'invoke', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'map', 'metadata', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'name', 'output_schema', 'parse_file', 'parse_obj', 'parse_raw', 'pick', 'pipe', 'schema', 'schema_json', 'search_kwargs', 'search_type', 'stream', 'tags', 'to_json', 'to_json_not_implemented', 'transform', 'update_forward_refs', 'validate', 'validate_search_type', 'vectorstore', 'with_alisteners', 'with_config', 'with_fallbacks', 'with_listeners', 'with_retry', 'with_types']\n",
            "Debug: Type of retriever: <class 'langchain_core.vectorstores.base.VectorStoreRetriever'>\n",
            "Debug: Attributes of retriever: ['InputType', 'OutputType', '__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__orig_bases__', '__parameters__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_on_complete__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_setattr_handlers__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__ror__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abatch_with_config', '_abc_impl', '_acall_with_config', '_aget_relevant_documents', '_atransform_stream_with_config', '_batch_with_config', '_calculate_keys', '_call_with_config', '_copy_and_set_values', '_expects_other_args', '_get_ls_params', '_get_relevant_documents', '_get_value', '_iter', '_new_arg_supported', '_setattr_handler', '_transform_stream_with_config', 'aadd_documents', 'abatch', 'abatch_as_completed', 'add_documents', 'ainvoke', 'allowed_search_types', 'as_tool', 'assign', 'astream', 'astream_events', 'astream_log', 'atransform', 'batch', 'batch_as_completed', 'bind', 'config_schema', 'config_specs', 'configurable_alternatives', 'configurable_fields', 'construct', 'copy', 'dict', 'from_orm', 'get_config_jsonschema', 'get_graph', 'get_input_jsonschema', 'get_input_schema', 'get_lc_namespace', 'get_name', 'get_output_jsonschema', 'get_output_schema', 'get_prompts', 'input_schema', 'invoke', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'map', 'metadata', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'name', 'output_schema', 'parse_file', 'parse_obj', 'parse_raw', 'pick', 'pipe', 'schema', 'schema_json', 'search_kwargs', 'search_type', 'stream', 'tags', 'to_json', 'to_json_not_implemented', 'transform', 'update_forward_refs', 'validate', 'validate_search_type', 'vectorstore', 'with_alisteners', 'with_config', 'with_fallbacks', 'with_listeners', 'with_retry', 'with_types']\n",
            "Debug: Type of retriever: <class 'langchain_core.vectorstores.base.VectorStoreRetriever'>\n",
            "Debug: Attributes of retriever: ['InputType', 'OutputType', '__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__orig_bases__', '__parameters__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_on_complete__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_setattr_handlers__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__ror__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abatch_with_config', '_abc_impl', '_acall_with_config', '_aget_relevant_documents', '_atransform_stream_with_config', '_batch_with_config', '_calculate_keys', '_call_with_config', '_copy_and_set_values', '_expects_other_args', '_get_ls_params', '_get_relevant_documents', '_get_value', '_iter', '_new_arg_supported', '_setattr_handler', '_transform_stream_with_config', 'aadd_documents', 'abatch', 'abatch_as_completed', 'add_documents', 'ainvoke', 'allowed_search_types', 'as_tool', 'assign', 'astream', 'astream_events', 'astream_log', 'atransform', 'batch', 'batch_as_completed', 'bind', 'config_schema', 'config_specs', 'configurable_alternatives', 'configurable_fields', 'construct', 'copy', 'dict', 'from_orm', 'get_config_jsonschema', 'get_graph', 'get_input_jsonschema', 'get_input_schema', 'get_lc_namespace', 'get_name', 'get_output_jsonschema', 'get_output_schema', 'get_prompts', 'input_schema', 'invoke', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'map', 'metadata', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'name', 'output_schema', 'parse_file', 'parse_obj', 'parse_raw', 'pick', 'pipe', 'schema', 'schema_json', 'search_kwargs', 'search_type', 'stream', 'tags', 'to_json', 'to_json_not_implemented', 'transform', 'update_forward_refs', 'validate', 'validate_search_type', 'vectorstore', 'with_alisteners', 'with_config', 'with_fallbacks', 'with_listeners', 'with_retry', 'with_types']\n",
            "Debug: Type of retriever: <class 'langchain_core.vectorstores.base.VectorStoreRetriever'>\n",
            "Debug: Attributes of retriever: ['InputType', 'OutputType', '__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__orig_bases__', '__parameters__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_on_complete__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_setattr_handlers__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__ror__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abatch_with_config', '_abc_impl', '_acall_with_config', '_aget_relevant_documents', '_atransform_stream_with_config', '_batch_with_config', '_calculate_keys', '_call_with_config', '_copy_and_set_values', '_expects_other_args', '_get_ls_params', '_get_relevant_documents', '_get_value', '_iter', '_new_arg_supported', '_setattr_handler', '_transform_stream_with_config', 'aadd_documents', 'abatch', 'abatch_as_completed', 'add_documents', 'ainvoke', 'allowed_search_types', 'as_tool', 'assign', 'astream', 'astream_events', 'astream_log', 'atransform', 'batch', 'batch_as_completed', 'bind', 'config_schema', 'config_specs', 'configurable_alternatives', 'configurable_fields', 'construct', 'copy', 'dict', 'from_orm', 'get_config_jsonschema', 'get_graph', 'get_input_jsonschema', 'get_input_schema', 'get_lc_namespace', 'get_name', 'get_output_jsonschema', 'get_output_schema', 'get_prompts', 'input_schema', 'invoke', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'map', 'metadata', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'name', 'output_schema', 'parse_file', 'parse_obj', 'parse_raw', 'pick', 'pipe', 'schema', 'schema_json', 'search_kwargs', 'search_type', 'stream', 'tags', 'to_json', 'to_json_not_implemented', 'transform', 'update_forward_refs', 'validate', 'validate_search_type', 'vectorstore', 'with_alisteners', 'with_config', 'with_fallbacks', 'with_listeners', 'with_retry', 'with_types']\n",
            "Debug: Type of retriever: <class 'langchain_core.vectorstores.base.VectorStoreRetriever'>\n",
            "Debug: Attributes of retriever: ['InputType', 'OutputType', '__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__orig_bases__', '__parameters__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_on_complete__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_setattr_handlers__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__ror__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abatch_with_config', '_abc_impl', '_acall_with_config', '_aget_relevant_documents', '_atransform_stream_with_config', '_batch_with_config', '_calculate_keys', '_call_with_config', '_copy_and_set_values', '_expects_other_args', '_get_ls_params', '_get_relevant_documents', '_get_value', '_iter', '_new_arg_supported', '_setattr_handler', '_transform_stream_with_config', 'aadd_documents', 'abatch', 'abatch_as_completed', 'add_documents', 'ainvoke', 'allowed_search_types', 'as_tool', 'assign', 'astream', 'astream_events', 'astream_log', 'atransform', 'batch', 'batch_as_completed', 'bind', 'config_schema', 'config_specs', 'configurable_alternatives', 'configurable_fields', 'construct', 'copy', 'dict', 'from_orm', 'get_config_jsonschema', 'get_graph', 'get_input_jsonschema', 'get_input_schema', 'get_lc_namespace', 'get_name', 'get_output_jsonschema', 'get_output_schema', 'get_prompts', 'input_schema', 'invoke', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'map', 'metadata', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'name', 'output_schema', 'parse_file', 'parse_obj', 'parse_raw', 'pick', 'pipe', 'schema', 'schema_json', 'search_kwargs', 'search_type', 'stream', 'tags', 'to_json', 'to_json_not_implemented', 'transform', 'update_forward_refs', 'validate', 'validate_search_type', 'vectorstore', 'with_alisteners', 'with_config', 'with_fallbacks', 'with_listeners', 'with_retry', 'with_types']\n",
            "Debug: Type of retriever: <class 'langchain_core.vectorstores.base.VectorStoreRetriever'>\n",
            "Debug: Attributes of retriever: ['InputType', 'OutputType', '__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__orig_bases__', '__parameters__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_on_complete__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_setattr_handlers__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__ror__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abatch_with_config', '_abc_impl', '_acall_with_config', '_aget_relevant_documents', '_atransform_stream_with_config', '_batch_with_config', '_calculate_keys', '_call_with_config', '_copy_and_set_values', '_expects_other_args', '_get_ls_params', '_get_relevant_documents', '_get_value', '_iter', '_new_arg_supported', '_setattr_handler', '_transform_stream_with_config', 'aadd_documents', 'abatch', 'abatch_as_completed', 'add_documents', 'ainvoke', 'allowed_search_types', 'as_tool', 'assign', 'astream', 'astream_events', 'astream_log', 'atransform', 'batch', 'batch_as_completed', 'bind', 'config_schema', 'config_specs', 'configurable_alternatives', 'configurable_fields', 'construct', 'copy', 'dict', 'from_orm', 'get_config_jsonschema', 'get_graph', 'get_input_jsonschema', 'get_input_schema', 'get_lc_namespace', 'get_name', 'get_output_jsonschema', 'get_output_schema', 'get_prompts', 'input_schema', 'invoke', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'map', 'metadata', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'name', 'output_schema', 'parse_file', 'parse_obj', 'parse_raw', 'pick', 'pipe', 'schema', 'schema_json', 'search_kwargs', 'search_type', 'stream', 'tags', 'to_json', 'to_json_not_implemented', 'transform', 'update_forward_refs', 'validate', 'validate_search_type', 'vectorstore', 'with_alisteners', 'with_config', 'with_fallbacks', 'with_listeners', 'with_retry', 'with_types']\n",
            "Debug: Type of retriever: <class 'langchain_core.vectorstores.base.VectorStoreRetriever'>\n",
            "Debug: Attributes of retriever: ['InputType', 'OutputType', '__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__orig_bases__', '__parameters__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_on_complete__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_setattr_handlers__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__ror__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abatch_with_config', '_abc_impl', '_acall_with_config', '_aget_relevant_documents', '_atransform_stream_with_config', '_batch_with_config', '_calculate_keys', '_call_with_config', '_copy_and_set_values', '_expects_other_args', '_get_ls_params', '_get_relevant_documents', '_get_value', '_iter', '_new_arg_supported', '_setattr_handler', '_transform_stream_with_config', 'aadd_documents', 'abatch', 'abatch_as_completed', 'add_documents', 'ainvoke', 'allowed_search_types', 'as_tool', 'assign', 'astream', 'astream_events', 'astream_log', 'atransform', 'batch', 'batch_as_completed', 'bind', 'config_schema', 'config_specs', 'configurable_alternatives', 'configurable_fields', 'construct', 'copy', 'dict', 'from_orm', 'get_config_jsonschema', 'get_graph', 'get_input_jsonschema', 'get_input_schema', 'get_lc_namespace', 'get_name', 'get_output_jsonschema', 'get_output_schema', 'get_prompts', 'input_schema', 'invoke', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'map', 'metadata', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'name', 'output_schema', 'parse_file', 'parse_obj', 'parse_raw', 'pick', 'pipe', 'schema', 'schema_json', 'search_kwargs', 'search_type', 'stream', 'tags', 'to_json', 'to_json_not_implemented', 'transform', 'update_forward_refs', 'validate', 'validate_search_type', 'vectorstore', 'with_alisteners', 'with_config', 'with_fallbacks', 'with_listeners', 'with_retry', 'with_types']\n",
            "Debug: Type of retriever: <class 'langchain_core.vectorstores.base.VectorStoreRetriever'>\n",
            "Debug: Attributes of retriever: ['InputType', 'OutputType', '__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__orig_bases__', '__parameters__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_on_complete__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_setattr_handlers__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__ror__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abatch_with_config', '_abc_impl', '_acall_with_config', '_aget_relevant_documents', '_atransform_stream_with_config', '_batch_with_config', '_calculate_keys', '_call_with_config', '_copy_and_set_values', '_expects_other_args', '_get_ls_params', '_get_relevant_documents', '_get_value', '_iter', '_new_arg_supported', '_setattr_handler', '_transform_stream_with_config', 'aadd_documents', 'abatch', 'abatch_as_completed', 'add_documents', 'ainvoke', 'allowed_search_types', 'as_tool', 'assign', 'astream', 'astream_events', 'astream_log', 'atransform', 'batch', 'batch_as_completed', 'bind', 'config_schema', 'config_specs', 'configurable_alternatives', 'configurable_fields', 'construct', 'copy', 'dict', 'from_orm', 'get_config_jsonschema', 'get_graph', 'get_input_jsonschema', 'get_input_schema', 'get_lc_namespace', 'get_name', 'get_output_jsonschema', 'get_output_schema', 'get_prompts', 'input_schema', 'invoke', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'map', 'metadata', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'name', 'output_schema', 'parse_file', 'parse_obj', 'parse_raw', 'pick', 'pipe', 'schema', 'schema_json', 'search_kwargs', 'search_type', 'stream', 'tags', 'to_json', 'to_json_not_implemented', 'transform', 'update_forward_refs', 'validate', 'validate_search_type', 'vectorstore', 'with_alisteners', 'with_config', 'with_fallbacks', 'with_listeners', 'with_retry', 'with_types']\n",
            "Debug: Type of retriever: <class 'langchain_core.vectorstores.base.VectorStoreRetriever'>\n",
            "Debug: Attributes of retriever: ['InputType', 'OutputType', '__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__orig_bases__', '__parameters__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_on_complete__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_setattr_handlers__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__ror__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abatch_with_config', '_abc_impl', '_acall_with_config', '_aget_relevant_documents', '_atransform_stream_with_config', '_batch_with_config', '_calculate_keys', '_call_with_config', '_copy_and_set_values', '_expects_other_args', '_get_ls_params', '_get_relevant_documents', '_get_value', '_iter', '_new_arg_supported', '_setattr_handler', '_transform_stream_with_config', 'aadd_documents', 'abatch', 'abatch_as_completed', 'add_documents', 'ainvoke', 'allowed_search_types', 'as_tool', 'assign', 'astream', 'astream_events', 'astream_log', 'atransform', 'batch', 'batch_as_completed', 'bind', 'config_schema', 'config_specs', 'configurable_alternatives', 'configurable_fields', 'construct', 'copy', 'dict', 'from_orm', 'get_config_jsonschema', 'get_graph', 'get_input_jsonschema', 'get_input_schema', 'get_lc_namespace', 'get_name', 'get_output_jsonschema', 'get_output_schema', 'get_prompts', 'input_schema', 'invoke', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'map', 'metadata', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'name', 'output_schema', 'parse_file', 'parse_obj', 'parse_raw', 'pick', 'pipe', 'schema', 'schema_json', 'search_kwargs', 'search_type', 'stream', 'tags', 'to_json', 'to_json_not_implemented', 'transform', 'update_forward_refs', 'validate', 'validate_search_type', 'vectorstore', 'with_alisteners', 'with_config', 'with_fallbacks', 'with_listeners', 'with_retry', 'with_types']\n",
            "Debug: Type of retriever: <class 'langchain_core.vectorstores.base.VectorStoreRetriever'>\n",
            "Debug: Attributes of retriever: ['InputType', 'OutputType', '__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__orig_bases__', '__parameters__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_on_complete__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_setattr_handlers__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__ror__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abatch_with_config', '_abc_impl', '_acall_with_config', '_aget_relevant_documents', '_atransform_stream_with_config', '_batch_with_config', '_calculate_keys', '_call_with_config', '_copy_and_set_values', '_expects_other_args', '_get_ls_params', '_get_relevant_documents', '_get_value', '_iter', '_new_arg_supported', '_setattr_handler', '_transform_stream_with_config', 'aadd_documents', 'abatch', 'abatch_as_completed', 'add_documents', 'ainvoke', 'allowed_search_types', 'as_tool', 'assign', 'astream', 'astream_events', 'astream_log', 'atransform', 'batch', 'batch_as_completed', 'bind', 'config_schema', 'config_specs', 'configurable_alternatives', 'configurable_fields', 'construct', 'copy', 'dict', 'from_orm', 'get_config_jsonschema', 'get_graph', 'get_input_jsonschema', 'get_input_schema', 'get_lc_namespace', 'get_name', 'get_output_jsonschema', 'get_output_schema', 'get_prompts', 'input_schema', 'invoke', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'map', 'metadata', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'name', 'output_schema', 'parse_file', 'parse_obj', 'parse_raw', 'pick', 'pipe', 'schema', 'schema_json', 'search_kwargs', 'search_type', 'stream', 'tags', 'to_json', 'to_json_not_implemented', 'transform', 'update_forward_refs', 'validate', 'validate_search_type', 'vectorstore', 'with_alisteners', 'with_config', 'with_fallbacks', 'with_listeners', 'with_retry', 'with_types']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                            question  \\\n",
              "0                  What is the gross margin for F19?   \n",
              "1  What is the number of stores under Dan Murphy'...   \n",
              "2  What is the company paid on a cost-plus type c...   \n",
              "3         What is the amount of total sales in 2019?   \n",
              "4  How is industry end market information presented?   \n",
              "5  In which years was for the net sales by segmen...   \n",
              "6  How is the discount rate for domestic plans de...   \n",
              "7  How is the discount rate for international pla...   \n",
              "8      What financial items are listed in the table?   \n",
              "9  Which countries does the group operate defined...   \n",
              "\n",
              "                                                gold  \\\n",
              "0                                            [22.9%]   \n",
              "1                                              [230]   \n",
              "2  [our allowable incurred costs plus a profit wh...   \n",
              "3                                         [$1,496.5]   \n",
              "4  [consistently with our internal management rep...   \n",
              "5                                 [2019, 2018, 2017]   \n",
              "6  [By comparison against the FTSE pension liabil...   \n",
              "7  [By comparison against country specific AA cor...   \n",
              "8  [Defined contribution schemes, Defined benefit...   \n",
              "9  [Germany, Ghana, India, Ireland, Italy, the UK...   \n",
              "\n",
              "                                                pred  em        f1  \n",
              "0                                              22.9%   1  1.000000  \n",
              "1                                                230   1  1.000000  \n",
              "2  On a cost-plus type contract, we are paid our ...   0  0.877193  \n",
              "3                                                 $0   0  0.000000  \n",
              "4  Industry end market information is presented c...   0  0.777778  \n",
              "5  The evidence provided does not specify the yea...   0  0.000000  \n",
              "6  For domestic plans, the discount rate was dete...   0  0.375000  \n",
              "7  The international discount rates were determin...   0  0.838710  \n",
              "8  The evidence provided does not contain any fin...   0  0.000000  \n",
              "9  The Group operates defined benefit schemes in ...   0  0.484848  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-76620740-51b8-4100-9502-45111e1a9924\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>gold</th>\n",
              "      <th>pred</th>\n",
              "      <th>em</th>\n",
              "      <th>f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the gross margin for F19?</td>\n",
              "      <td>[22.9%]</td>\n",
              "      <td>22.9%</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the number of stores under Dan Murphy'...</td>\n",
              "      <td>[230]</td>\n",
              "      <td>230</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What is the company paid on a cost-plus type c...</td>\n",
              "      <td>[our allowable incurred costs plus a profit wh...</td>\n",
              "      <td>On a cost-plus type contract, we are paid our ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.877193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is the amount of total sales in 2019?</td>\n",
              "      <td>[$1,496.5]</td>\n",
              "      <td>$0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How is industry end market information presented?</td>\n",
              "      <td>[consistently with our internal management rep...</td>\n",
              "      <td>Industry end market information is presented c...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.777778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>In which years was for the net sales by segmen...</td>\n",
              "      <td>[2019, 2018, 2017]</td>\n",
              "      <td>The evidence provided does not specify the yea...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How is the discount rate for domestic plans de...</td>\n",
              "      <td>[By comparison against the FTSE pension liabil...</td>\n",
              "      <td>For domestic plans, the discount rate was dete...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.375000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How is the discount rate for international pla...</td>\n",
              "      <td>[By comparison against country specific AA cor...</td>\n",
              "      <td>The international discount rates were determin...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.838710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>What financial items are listed in the table?</td>\n",
              "      <td>[Defined contribution schemes, Defined benefit...</td>\n",
              "      <td>The evidence provided does not contain any fin...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Which countries does the group operate defined...</td>\n",
              "      <td>[Germany, Ghana, India, Ireland, Italy, the UK...</td>\n",
              "      <td>The Group operates defined benefit schemes in ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.484848</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-76620740-51b8-4100-9502-45111e1a9924')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-76620740-51b8-4100-9502-45111e1a9924 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-76620740-51b8-4100-9502-45111e1a9924');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(f\\\"Average F1: {df_results['f1']\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"What financial items are listed in the table?\",\n          \"What is the number of stores under Dan Murphy's store network in F19?\",\n          \"In which years was for the net sales by segment and industry end market calculated?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gold\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"The evidence provided does not contain any financial items listed in the table.\",\n          \"230\",\n          \"The evidence provided does not specify the years for which the net sales by segment and industry end market were calculated.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"em\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.41998883370417034,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          1.0,\n          0.8771929824561403\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average EM: 0.20\n",
            "Average F1: 0.54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ap-3efhPQrwz",
        "outputId": "63c2a498-811b-4f45-fb6d-b1874a2be00a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(gold[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F654Hh0sVahc",
        "outputId": "594ac923-c8df-4a82-d0a0-052356ec6b77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22.9%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = dev_data[0]\n",
        "print(sample)\n",
        "for i, sample in enumerate(dev_data[:3]):\n",
        "    for q in sample[\"questions\"][:1]:  # take first question per sample\n",
        "\n",
        "      print(q['answer'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1o4UtWx-pHr",
        "outputId": "f9c254ab-4288-4bbb-ee4a-f2c7af004133"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'table': {'uid': '00addb09-bf31-4240-a351-bc4480a3cecc', 'table': [['', 'F19', 'F18 (3)', '', 'CHANGE'], ['$ MILLION', '53 WEEKS', '52 WEEKS', 'CHANGE', 'NORMALISED'], ['Sales', '8,657', '8,244', '5.0%', '3.2%'], ['EBITDA', '579', '603', '(4.1)%', '(5.4)%'], ['Depreciation and amortisation', '(105)', '(87)', '20.1%', '20.1%'], ['EBIT', '474', '516', '(8.2)%', '(9.7)%'], ['Gross margin (%)', '22.9', '23.1', '(16) bps', '(14) bps'], ['Cost of doing business (%)', '17.4', '16.8', '63 bps', '64 bps'], ['EBIT to sales (%)', '5.5', '6.3', '(78) bps', '(78) bps'], ['Sales per square metre ($)$)', '18,675', '18,094', '3.2%', '1.4%'], ['Funds employed', '3,185', '3,214', '(0.9)%', ''], ['ROFE (%)', '15.2', '17.1', '(190) bps', '(215) bps']]}, 'paragraphs': [{'uid': 'fd8bba97-948c-4afa-9469-9027df970c45', 'order': 1, 'text': 'In Endeavour Drinks, BWS and Dan Murphy’s key VOC metrics ended F19 at record highs, with improvements both in store and Online. Sales increased by 5.0% (3.2% normalised) to $8.7 billion with comparable sales increasing 2.3%. The market remained subdued throughout the year with declining volumes offset by price and mix improvements. Sales growth in H2 improved on H1 in both Dan Murphy’s and BWS, with Endeavour Drinks’ sales increasing by 4.8% (normalised) with comparable sales increasing 4.0%, compared to 0.7% growth in H1. The timing of New Year’s Day boosted sales in H2 by 84 bps and Q3, in particular, also benefitted from more stable weather compared to Q2. Dan Murphy’s focus on ‘discovery’ driven range, service and convenience is also beginning to resonate with customers.'}, {'uid': '934b3a31-5b30-4323-a093-b342eb0cb3e1', 'order': 2, 'text': 'BWS maintained its strong trading momentum, with enhancements to localised ranging and tailored Woolworths Rewards offerings. The BWS store network grew to 1,346 stores with 30 net new stores and the new BWS Renewal format successfully extended to key urban standalone stores. BWS’ convenience offering continued to expand, with On Demand delivery now available in 605 stores, supporting double digit online sales growth. Jimmy Brings expanded its geographical reach to Brisbane, Gold Coast, Canberra and new suburbs in Sydney and Melbourne.'}, {'uid': 'fd5eddfd-156b-441b-b608-7e79d5864946', 'order': 3, 'text': 'Dan Murphy’s delivered double digit Online sales growth with new customer offerings, including the roll out of On Demand delivery to 91 stores and 30‐minute Pick up from all stores. In store customer experience was enhanced with the introduction of wine merchants in key stores, to improve team product knowledge and customer discovery, while memberships in My Dan’s loyalty program increased 15% on the prior year. Dan Murphy’s store network grew to 230 with three new store openings in Q4 including the first store to be powered by solar energy.'}, {'uid': '243dda1c-30f7-4bf5-873d-5f4e7f58f56b', 'order': 4, 'text': 'Endeavour Drinks sales per square metre increased by 3.2% (1.4% normalised) with sales growth above net average space growth of 1.7%.'}, {'uid': '444df807-dd92-4aba-9cf0-c3e5052145ca', 'order': 5, 'text': 'Gross margin was 22.9%, 14 bps down on a normalised basis, with trading margin improvements offset by higher freight costs attributable to petrol prices, growth in online delivery and category mix.'}, {'uid': 'dfdf543e-e2d8-4d41-9a66-74044ccc3251', 'order': 6, 'text': 'Normalised CODB as a percentage of sales grew 64 bps, driven by a $21 million impairment charge related to goodwill and other intangible assets associated with the Summergate business in China. Summergate has now transitioned to ExportCo. Excluding Summergate, normalised CODB as a percentage of sales increased by 40 bps due to above inflationary cost pressures, as well as targeted investment in key focus areas including customer experience, ranging, data and analytics.'}, {'uid': '519d9e35-2d12-4a61-9705-5bde3a62369d', 'order': 7, 'text': 'Endeavour Drinks EBIT for F19 decreased 8.2% to $474 million. EBIT normalised for the 53rd week and Summergate impairment of $21 million decreased 5.6%. Normalised ROFE (excluding the Summergate impairment) declined 148 bps driven by the decline in EBIT.'}, {'uid': '47a4e9d4-f16b-44e8-99f8-b10115127b73', 'order': 8, 'text': '(3) During the period, the management of the New Zealand Wine Cellars business transferred from Endeavour Drinks to New Zealand Food. The prior period has been re‑presented toconform with the current period presentation.'}], 'questions': [{'uid': 'fbd46bde-fdbe-4926-b24a-aff8a2487a3d', 'order': 1, 'question': 'What is the gross margin for F19?', 'answer': ['22.9%'], 'derivation': '', 'answer_type': 'span', 'answer_from': 'table-text', 'facts': ['22.9%'], 'mapping': {'table': [], 'paragraph': {'5': [[16, 22]]}}, 'rel_paragraphs': ['5'], 'req_comparison': False, 'scale': ''}, {'uid': 'a2d7d0c0-0939-4563-95db-e93199c38e13', 'order': 2, 'question': \"What is the number of stores under Dan Murphy's store network in F19?\", 'answer': ['230'], 'derivation': '', 'answer_type': 'span', 'answer_from': 'text', 'facts': ['230'], 'mapping': {'paragraph': {'3': [[451, 454]]}}, 'rel_paragraphs': ['3'], 'req_comparison': False, 'scale': ''}, {'uid': '5b290c22-ff6b-405a-aeb7-1277c22c2df4', 'order': 3, 'question': 'What percentage did Sales increased by between F19 and F18?', 'answer': ['5.0%'], 'derivation': '', 'answer_type': 'span', 'answer_from': 'table-text', 'facts': ['5.0%'], 'mapping': {'table': [[2, 3]]}, 'rel_paragraphs': ['1'], 'req_comparison': False, 'scale': ''}, {'uid': 'c91ad938-6ad1-4481-8f6b-43ccd2f69926', 'order': 4, 'question': 'What is the nominal difference for ROFE between F19 and F18?', 'answer': 1.9, 'derivation': '17.1    -    15.2   ', 'answer_type': 'arithmetic', 'answer_from': 'table', 'facts': ['15.2', '17.1'], 'mapping': {'table': [[11, 1], [11, 2]]}, 'rel_paragraphs': [], 'req_comparison': False, 'scale': 'percent'}, {'uid': '3c257d89-4281-43df-998a-f7850b4fda9c', 'order': 5, 'question': 'What is the average value for sales per square metre for both F19 and F18?', 'answer': 18384.5, 'derivation': ' ( 18,675    +    18,094 )  / 2   ', 'answer_type': 'arithmetic', 'answer_from': 'table', 'facts': ['18,094', '18,675'], 'mapping': {'table': [[9, 2], [9, 1]]}, 'rel_paragraphs': [], 'req_comparison': False, 'scale': 'million'}, {'uid': '6586d5cf-16c3-444a-b946-f8485a5c2a82', 'order': 6, 'question': 'What is the average ROFE for the years F19 and F18?', 'answer': 16.15, 'derivation': ' ( 15.2    +    17.1 )  / 2   ', 'answer_type': 'arithmetic', 'answer_from': 'table', 'facts': ['15.2', '17.1'], 'mapping': {'table': [[11, 1], [11, 2]]}, 'rel_paragraphs': [], 'req_comparison': False, 'scale': 'percent'}]}\n",
            "22.9%\n",
            "our allowable incurred costs plus a profit which can be fixed or variable depending on the contract’s fee arrangement up to predetermined funding levels determined by the customer\n",
            "consistently with our internal management reporting and may be revised periodically as management deems necessary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Helper to safely convert answer to list of strings\n",
        "def get_answer_strings(answer):\n",
        "    if isinstance(answer, list):\n",
        "        return [str(a) for a in answer]\n",
        "    else:\n",
        "        return [str(answer)]\n",
        "\n",
        "# Load dev data\n",
        "dev_data = load_tatqa_split(\"tatqa_dataset_dev\")\n",
        "\n",
        "num_samples = 5          # number of samples to evaluate\n",
        "questions_per_sample = 3 # questions per sample\n",
        "k = 3                    # top-k for retrieval metrics\n",
        "results = []\n",
        "\n",
        "for i, sample in enumerate(dev_data[:num_samples]):\n",
        "    # --- Build retriever for this sample ---\n",
        "    paragraphs = [p[\"text\"] for p in sample[\"paragraphs\"]]\n",
        "    # Use string IDs in metadata\n",
        "    metadatas = [{\"order\": str(j)} for j, p in enumerate(sample[\"paragraphs\"])]\n",
        "    vectorstore = Chroma.from_texts(\n",
        "        texts=paragraphs,\n",
        "        embedding=embeddings,\n",
        "        metadatas=metadatas\n",
        "    )\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
        "\n",
        "    # --- Determine gold paragraph IDs for the first question ---\n",
        "    q = sample[\"questions\"][0]\n",
        "    gold = q[\"answer\"]\n",
        "    gold_para_ids = []\n",
        "    for j, p in enumerate(sample[\"paragraphs\"]):\n",
        "        for g_str in get_answer_strings(gold):\n",
        "            if g_str.lower() in p[\"text\"].lower():\n",
        "                gold_para_ids.append(str(j))\n",
        "                break   # once matched, move to next paragraph\n",
        "\n",
        "    # Process the question\n",
        "    question = q[\"question\"]\n",
        "    gold = q[\"answer\"]\n",
        "\n",
        "    # Prepare initial state\n",
        "    state = {\n",
        "        \"question\": question,\n",
        "        \"table_headers\": sample[\"table\"][\"table\"][0],\n",
        "        \"table_rows\": sample[\"table\"][\"table\"][1:],\n",
        "        \"paragraphs\": paragraphs,\n",
        "        \"retrieved_texts\": [],\n",
        "        \"retrieved_paragraph_ids\": [],\n",
        "        \"extracted_numbers\": [],\n",
        "        \"plan\": [],\n",
        "        \"intermediate_results\": {},\n",
        "        \"final_answer\": None,\n",
        "        \"error\": None\n",
        "    }\n",
        "\n",
        "    # Run the graph\n",
        "    out = app.invoke(state, config={\"configurable\": {\"thread_id\": f\"eval_{i}\"}})\n",
        "\n",
        "    pred = out[\"final_answer\"]\n",
        "    retrieved_ids = [str(x) for x in out.get(\"retrieved_paragraph_ids\", [])]  # force strings\n",
        "\n",
        "    # Compute retrieval metrics\n",
        "    ret_metrics = compute_retrieval_metrics(retrieved_ids, gold_para_ids, k=k)\n",
        "\n",
        "    # Compute generation metrics\n",
        "    em = compute_em(pred, gold)\n",
        "    f1 = compute_f1(pred, gold)\n",
        "    pred_str = normalize_answer(pred)\n",
        "    gold_str = normalize_answer(gold)\n",
        "    rouge_scores = compute_rouge(pred_str, gold_str)\n",
        "    bleu = compute_bleu(pred_str, gold_str)\n",
        "    cosine_sim = compute_cosine_similarity(pred_str, gold_str)\n",
        "\n",
        "    # Assemble row\n",
        "    row = {\n",
        "        \"question\": question,\n",
        "        \"gold\": gold,\n",
        "        \"pred\": pred,\n",
        "        \"em\": em,\n",
        "        \"f1\": f1,\n",
        "        \"rouge1_f1\": rouge_scores['rouge1_f1'],\n",
        "        \"bleu\": bleu,\n",
        "        \"cosine_sim\": cosine_sim,\n",
        "    }\n",
        "    row.update(ret_metrics)   # adds precision@k, recall@k, map\n",
        "    results.append(row)\n",
        "\n",
        "# Create DataFrame and display\n",
        "df_results = pd.DataFrame(results)\n",
        "display(df_results[[\"question\", \"gold\", \"pred\", \"em\", \"f1\", \"rouge1_f1\", \"bleu\", \"cosine_sim\",\n",
        "                    f\"precision@{k}\", f\"recall@{k}\", \"map\"]])\n",
        "\n",
        "# Print averages\n",
        "print(f\"Average EM: {df_results['em'].mean():.2f}\")\n",
        "print(f\"Average F1: {df_results['f1'].mean():.2f}\")\n",
        "print(f\"Average ROUGE-1 F1: {df_results['rouge1_f1'].mean():.2f}\")\n",
        "print(f\"Average BLEU: {df_results['bleu'].mean():.2f}\")\n",
        "print(f\"Average Cosine Similarity: {df_results['cosine_sim'].mean():.2f}\")\n",
        "print(f\"Average precision@{k}: {df_results[f'precision@{k}'].mean():.2f}\")\n",
        "print(f\"Average recall@{k}: {df_results[f'recall@{k}'].mean():.2f}\")\n",
        "print(f\"Average map: {df_results['map'].mean():.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        },
        "id": "QRdiv8TKUAF0",
        "outputId": "d4f20315-23ef-4497-a5eb-1a489fc65a4d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: Entering text_retrieval_agent\n",
            "Debug: Retrieved 3 paragraph IDs: [4, 4, 5]\n",
            "Debug: Entering text_retrieval_agent\n",
            "Debug: Retrieved 3 paragraph IDs: [1, '1', '1']\n",
            "Debug: Entering text_retrieval_agent\n",
            "Debug: Retrieved 3 paragraph IDs: [1, 1, '1']\n",
            "Debug: Entering text_retrieval_agent\n",
            "Debug: Retrieved 3 paragraph IDs: ['1', 1, '1']\n",
            "Debug: Entering text_retrieval_agent\n",
            "Debug: Retrieved 3 paragraph IDs: [2, 2, 2]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                            question  \\\n",
              "0                  What is the gross margin for F19?   \n",
              "1  What is the company paid on a cost-plus type c...   \n",
              "2  How is industry end market information presented?   \n",
              "3  How is the discount rate for domestic plans de...   \n",
              "4      What financial items are listed in the table?   \n",
              "\n",
              "                                                gold  \\\n",
              "0                                            [22.9%]   \n",
              "1  [our allowable incurred costs plus a profit wh...   \n",
              "2  [consistently with our internal management rep...   \n",
              "3  [By comparison against the FTSE pension liabil...   \n",
              "4  [Defined contribution schemes, Defined benefit...   \n",
              "\n",
              "                                                pred  em   f1  rouge1_f1  \\\n",
              "0                                              22.9%   1  1.0        1.0   \n",
              "1  We are paid our allowable incurred costs plus ...   0  0.0        0.0   \n",
              "2  Industry end market information is presented c...   0  0.0        0.0   \n",
              "3  For domestic plans, the discount rate was dete...   0  0.0        0.0   \n",
              "4  The evidence provided does not contain any fin...   0  0.0        0.0   \n",
              "\n",
              "       bleu  cosine_sim  precision@3  recall@3     map  \n",
              "0  0.177828    1.000000     0.333333    1.0000  2.0000  \n",
              "1  0.000000    1.000000     0.333333    1.0000  3.0000  \n",
              "2  0.000000    0.051055     0.333333    1.0000  3.0000  \n",
              "3  0.000000    0.024606     0.333333    1.0000  3.0000  \n",
              "4  0.000000    0.274922     0.333333    0.0625  0.1875  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c349cd2b-1f21-449a-a5df-a27bc5a0b67b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>gold</th>\n",
              "      <th>pred</th>\n",
              "      <th>em</th>\n",
              "      <th>f1</th>\n",
              "      <th>rouge1_f1</th>\n",
              "      <th>bleu</th>\n",
              "      <th>cosine_sim</th>\n",
              "      <th>precision@3</th>\n",
              "      <th>recall@3</th>\n",
              "      <th>map</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the gross margin for F19?</td>\n",
              "      <td>[22.9%]</td>\n",
              "      <td>22.9%</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.177828</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>2.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the company paid on a cost-plus type c...</td>\n",
              "      <td>[our allowable incurred costs plus a profit wh...</td>\n",
              "      <td>We are paid our allowable incurred costs plus ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>3.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How is industry end market information presented?</td>\n",
              "      <td>[consistently with our internal management rep...</td>\n",
              "      <td>Industry end market information is presented c...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.051055</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>3.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How is the discount rate for domestic plans de...</td>\n",
              "      <td>[By comparison against the FTSE pension liabil...</td>\n",
              "      <td>For domestic plans, the discount rate was dete...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.024606</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>3.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What financial items are listed in the table?</td>\n",
              "      <td>[Defined contribution schemes, Defined benefit...</td>\n",
              "      <td>The evidence provided does not contain any fin...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.274922</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0625</td>\n",
              "      <td>0.1875</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c349cd2b-1f21-449a-a5df-a27bc5a0b67b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c349cd2b-1f21-449a-a5df-a27bc5a0b67b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c349cd2b-1f21-449a-a5df-a27bc5a0b67b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(f\\\"Average map: {df_results['map']\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"What is the company paid on a cost-plus type contract?\",\n          \"What financial items are listed in the table?\",\n          \"How is industry end market information presented?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gold\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"We are paid our allowable incurred costs plus a profit which can be fixed or variable depending on the contract\\u2019s fee arrangement up to predetermined funding levels determined by the customer.\",\n          \"The evidence provided does not contain any financial items listed in the table.\",\n          \"Industry end market information is presented consistently with our internal management reporting and may be revised periodically as management deems necessary.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"em\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.44721359549995804,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rouge1_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.44721359549995804,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07952707287670507,\n        \"min\": 0.0,\n        \"max\": 0.1778279410038923,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          0.1778279410038923\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cosine_sim\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4933928697322367,\n        \"min\": 0.024606117978692055,\n        \"max\": 1.0000001192092896,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1.0000001192092896,\n          0.2749215364456177\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precision@3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.3333333333333333,\n        \"max\": 0.3333333333333333,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.3333333333333333\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall@3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4192627457812106,\n        \"min\": 0.0625,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"map\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.2250637738501617,\n        \"min\": 0.1875,\n        \"max\": 3.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average EM: 0.20\n",
            "Average F1: 0.20\n",
            "Average ROUGE-1 F1: 0.20\n",
            "Average BLEU: 0.04\n",
            "Average Cosine Similarity: 0.47\n",
            "Average precision@3: 0.33\n",
            "Average recall@3: 0.81\n",
            "Average map: 2.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enhancing Information Retrieval for Table-Centered Question Answering with Text in Technical Documents\n",
        "\n",
        "This repository contains the code and resources for the M.Sc. thesis project by **Ghazaleh Keivani Heshajani** at the University of Isfahan, Shahreza Campus, under the supervision of Dr. Maryam Lotfi and Dr. Maryam Hosseini.\n",
        "\n",
        "## 📌 Project Overview\n",
        "\n",
        "Technical documents in industrial domains (e.g., steel, oil & gas, electronics) often contain a mix of **semi‑structured tables** and **unstructured text**. Current knowledge management systems and search engines fail to effectively retrieve and reason over such heterogeneous data, leading to costly reliance on human memory and delayed operational decisions.\n",
        "\n",
        "This research aims to **enhance information retrieval for question answering over hybrid technical documents** by combining:\n",
        "\n",
        "- **Agentic AI** – multi‑agent systems that plan, reason, and use external tools.\n",
        "- **Retrieval‑Augmented Generation (RAG)** – integrating vector and graph‑based retrieval.\n",
        "- **Knowledge Graphs** – capturing implicit relationships between tables and text.\n",
        "- **Multimodal understanding** – processing tables, text, and potentially images/layout.\n",
        "\n",
        "We focus on the **TAT‑QA dataset** – a real‑world financial QA benchmark requiring numerical reasoning over tables and associated paragraphs. Our approach will be evaluated on similar industrial‑style documents.\n",
        "\n",
        "## 🎯 Objectives\n",
        "\n",
        "1. **Extract and align** complex tables and surrounding text from noisy technical documents using layout‑aware models (e.g., DocLLM, LayoutLM).\n",
        "2. **Design a multi‑agent architecture** (inspired by MAPLE, MACT, G‑MACT) where specialised agents handle planning, retrieval, computation, and verification.\n",
        "3. **Implement hybrid RAG** that combines dense vector retrieval with knowledge graph traversal to retrieve relevant table cells and text spans.\n",
        "4. **Answer complex multi‑step questions** requiring numerical reasoning (addition, subtraction, comparison, aggregation) over combined evidence.\n",
        "5. **Evaluate** on TAT‑QA and possibly a custom industrial dataset using Exact Match, F1, and execution accuracy.\n",
        "\n",
        "## 🧠 Methodology\n",
        "\n",
        "The proposed system is built on **LangGraph** for agent orchestration and **LangChain** for tool integration. The main workflow consists of:\n",
        "\n",
        "- **Planning Agent**: decomposes the question into a sequence of sub‑tasks (e.g., filter rows, retrieve text, compute ratio).\n",
        "- **Table Parser Agent**: extracts structured information from tables (handles multi‑level headers, merged cells).\n",
        "- **Text Retriever Agent**: performs dense retrieval over the associated paragraphs.\n",
        "- **Computation Agent**: writes and executes Python code for numerical operations.\n",
        "- **Verification Agent**: checks the consistency of the answer with the evidence.\n",
        "- **Memory**: a long‑term memory (via LangGraph state) stores successful reasoning paths for future reuse.\n",
        "\n",
        "We experiment with both **single‑agent** (ReAct) and **multi‑agent** (MAPLE‑like) architectures, and compare **vector‑only RAG** with **graph‑enhanced RAG**.\n",
        "\n",
        "## 📊 Dataset\n",
        "\n",
        "Primary dataset: **[TAT‑QA](https://github.com/NExTplusplus/TAT-QA)**  \n",
        "- 16,552 QA pairs over 2,757 tables from real financial reports.\n",
        "- Each sample contains a table (with row/col headers and numbers) and ≥2 descriptive paragraphs.\n",
        "- Questions require numerical reasoning (addition, subtraction, multiplication, division, counting, comparison) and often combine table and text.\n",
        "- Answer types: span, multi‑span, arithmetic expression, counting.\n",
        "\n",
        "We will also explore extending the evaluation to scientific tables (SciTab) or custom industrial documents if available.\n",
        "\n",
        "## 🛠️ Tech Stack\n",
        "\n",
        "- **Python 3.10+**\n",
        "- **LangChain** – tool definitions, RAG pipelines\n",
        "- **LangGraph** – multi‑agent state graphs\n",
        "- **HuggingFace Transformers** – for layout‑aware models (e.g., LayoutLMv3, DocLLM)\n",
        "- **FAISS / Chroma** – vector stores\n",
        "- **NetworkX / Neo4j** – knowledge graph storage and querying\n",
        "- **Pandas / NumPy** – data manipulation\n",
        "- **OpenAI / LLaMA / Qwen** – LLM backends (open‑weight models preferred)\n",
        "\n",
        "## 📁 Repository Structure\n",
        "\n",
        "```\n",
        ".\n",
        "├── agents/               # Agent definitions (planner, parser, retriever, etc.)\n",
        "├── data/                 # Dataset loaders and preprocessors (TAT‑QA)\n",
        "├── graphs/               # LangGraph workflow definitions\n",
        "├── retrieval/            # Dense and graph‑based retrieval modules\n",
        "├── evaluation/           # Metrics (EM, F1, execution accuracy)\n",
        "├── notebooks/            # Exploratory analysis and demos\n",
        "├── configs/              # Configuration files (LLM endpoints, embedding models)\n",
        "├── requirements.txt\n",
        "└── README.md\n",
        "```\n",
        "\n",
        "## 🚀 Getting Started\n",
        "\n",
        "1. Clone the repository:\n",
        "   ```bash\n",
        "   git clone https://github.com/yourusername/table-text-qa-agent.git\n",
        "   cd table-text-qa-agent\n",
        "   ```\n",
        "\n",
        "2. Install dependencies:\n",
        "   ```bash\n",
        "   pip install -r requirements.txt\n",
        "   ```\n",
        "\n",
        "3. Download the TAT‑QA dataset (see `data/README.md` for instructions).\n",
        "\n",
        "4. Run a baseline experiment:\n",
        "   ```bash\n",
        "   python run_experiment.py --config configs/baseline.yaml\n",
        "   ```\n",
        "\n",
        "5. (Optional) Set up a Neo4j instance for graph‑based retrieval.\n",
        "\n",
        "## 📈 Evaluation Metrics\n",
        "\n",
        "We will report:\n",
        "\n",
        "- **Exact Match (EM)** – for answers that must match the ground truth exactly.\n",
        "- **F1 Score** – for multi‑span and textual answers.\n",
        "- **Execution Accuracy** – for answers produced by generated code.\n",
        "- **Latency** – average time per query.\n",
        "- **Token consumption** – to compare cost against API‑based models.\n",
        "\n",
        "## 📄 Related Work\n",
        "\n",
        "- **TAT‑QA**: Zhu et al. (2021) – dataset.\n",
        "- **MAPLE**: Bai et al. (2025) – multi‑agent adaptive planning with memory.\n",
        "- **MACT / G‑MACT**: Zhou et al. (2025) – efficient multi‑agent collaboration with tool use.\n",
        "- **ReAcTable**: Zhang et al. (2024) – enhancing ReAct for table QA.\n",
        "- **DocLLM**: Wang et al. (2023) – layout‑aware document LLM.\n",
        "\n",
        "## 👥 Supervision\n",
        "\n",
        "- **Dr. Maryam Lotfi** (Primary Supervisor) – Shahreza Higher Education Center\n",
        "- **Dr. Maryam Hosseini** (Co‑Supervisor) – Shahreza Higher Education Center\n",
        "\n",
        "## 📝 License\n",
        "\n",
        "This project is for academic research purposes. The TAT‑QA dataset is available for non‑commercial use under its own license.\n",
        "\n",
        "## 📬 Contact\n",
        "\n",
        "For questions or collaboration, please contact:  \n",
        "Ghazaleh Keivani Heshajani – [email address]  \n",
        "University of Isfahan, Shahreza Campus\n",
        "\n",
        "---\n",
        "\n",
        "**Note:** This repository is under active development for a master's thesis. Results and code will be updated as the research progresses."
      ],
      "metadata": {
        "id": "qr-xKmGPEo7U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```markdown\n",
        "# Enhancing Information Retrieval for Table-Centered Question Answering with Text in Technical Documents\n",
        "\n",
        "This repository contains the code and resources for the M.Sc. thesis project by **Ghazaleh Keivani Heshajani** at the University of Isfahan, Shahreza Campus, under the supervision of Dr. Maryam Lotfi and Dr. Maryam Hosseini.\n",
        "\n",
        "## 📌 Project Overview\n",
        "\n",
        "Technical documents in industrial domains (e.g., steel, oil & gas, electronics) often contain a mix of **semi‑structured tables** and **unstructured text**. Current knowledge management systems and search engines fail to effectively retrieve and reason over such heterogeneous data, leading to costly reliance on human memory and delayed operational decisions.\n",
        "\n",
        "This research aims to **enhance information retrieval for question answering over hybrid technical documents** by combining:\n",
        "\n",
        "- **Agentic AI** – multi‑agent systems that plan, reason, and use external tools.\n",
        "- **Retrieval‑Augmented Generation (RAG)** – integrating vector and graph‑based retrieval.\n",
        "- **Knowledge Graphs** – capturing implicit relationships between tables and text.\n",
        "- **Multimodal understanding** – processing tables, text, and potentially images/layout.\n",
        "\n",
        "We focus on the **TAT‑QA dataset** – a real‑world financial QA benchmark requiring numerical reasoning over tables and associated paragraphs. Our approach will be evaluated on similar industrial‑style documents.\n",
        "\n",
        "## 🎯 Objectives\n",
        "\n",
        "1. **Extract and align** complex tables and surrounding text from noisy technical documents using layout‑aware models (e.g., DocLLM, LayoutLM).\n",
        "2. **Design a multi‑agent architecture** (inspired by MAPLE, MACT, G‑MACT) where specialised agents handle planning, retrieval, computation, and verification.\n",
        "3. **Implement hybrid RAG** that combines dense vector retrieval with knowledge graph traversal to retrieve relevant table cells and text spans.\n",
        "4. **Answer complex multi‑step questions** requiring numerical reasoning (addition, subtraction, comparison, aggregation) over combined evidence.\n",
        "5. **Evaluate** on TAT‑QA and possibly a custom industrial dataset using a comprehensive set of metrics.\n",
        "\n",
        "## 🧠 Methodology\n",
        "\n",
        "The proposed system is built on **LangGraph** for agent orchestration and **LangChain** for tool integration. The main workflow consists of:\n",
        "\n",
        "- **Planning Agent**: decomposes the question into a sequence of sub‑tasks (e.g., filter rows, retrieve text, compute ratio).\n",
        "- **Table Parser Agent**: extracts structured information from tables (handles multi‑level headers, merged cells).\n",
        "- **Text Retriever Agent**: performs dense retrieval over the associated paragraphs (using a per‑sample vector store).\n",
        "- **Computation Agent**: writes and executes Python code for numerical operations.\n",
        "- **Verification Agent**: checks the consistency of the answer with the evidence and produces the final answer.\n",
        "- **Memory**: a short‑term memory (via LangGraph state) stores intermediate results and reasoning steps.\n",
        "\n",
        "We experiment with both **single‑agent** (ReAct) and **multi‑agent** (MAPLE‑like) architectures, and compare **vector‑only RAG** with **graph‑enhanced RAG**.\n",
        "\n",
        "## 📊 Dataset\n",
        "\n",
        "Primary dataset: **[TAT‑QA](https://github.com/NExTplusplus/TAT-QA)**  \n",
        "- 16,552 QA pairs over 2,757 tables from real financial reports.\n",
        "- Each sample contains a table (with row/col headers and numbers) and ≥2 descriptive paragraphs.\n",
        "- Questions require numerical reasoning (addition, subtraction, multiplication, division, counting, comparison) and often combine table and text.\n",
        "- Answer types: span, multi‑span, arithmetic expression, counting.\n",
        "\n",
        "We also plan to explore scientific tables (SciTab) or custom industrial documents.\n",
        "\n",
        "## 🛠️ Tech Stack\n",
        "\n",
        "- **Python 3.10+**\n",
        "- **LangChain** – tool definitions, RAG pipelines\n",
        "- **LangGraph** – multi‑agent state graphs\n",
        "- **HuggingFace Transformers** – for layout‑aware models (e.g., LayoutLMv3, DocLLM)\n",
        "- **FAISS / Chroma** – vector stores\n",
        "- **NetworkX / Neo4j** – knowledge graph storage and querying\n",
        "- **Pandas / NumPy** – data manipulation\n",
        "- **OpenAI / LLaMA / Qwen** – LLM backends (open‑weight models preferred)\n",
        "- **Evaluation libraries**: ROUGE, BLEU, sentence‑transformers, scikit‑learn\n",
        "\n",
        "## 📁 Repository Structure\n",
        "\n",
        "```\n",
        ".\n",
        "├── agents/               # Agent definitions (planner, parser, retriever, etc.)\n",
        "├── data/                 # Dataset loaders and preprocessors (TAT‑QA)\n",
        "├── graphs/               # LangGraph workflow definitions\n",
        "├── retrieval/            # Dense and graph‑based retrieval modules\n",
        "├── evaluation/           # Metrics (EM, F1, ROUGE, BLEU, precision@k, recall@k, MAP)\n",
        "├── notebooks/            # Exploratory analysis and demos (including evaluation notebooks)\n",
        "├── configs/              # Configuration files (LLM endpoints, embedding models)\n",
        "├── requirements.txt\n",
        "└── README.md\n",
        "```\n",
        "\n",
        "## 🚀 Getting Started\n",
        "\n",
        "1. **Clone the repository**:\n",
        "   ```bash\n",
        "   git clone https://github.com/yourusername/table-text-qa-agent.git\n",
        "   cd table-text-qa-agent\n",
        "   ```\n",
        "\n",
        "2. **Install dependencies**:\n",
        "   ```bash\n",
        "   pip install -r requirements.txt\n",
        "   ```\n",
        "\n",
        "3. **Download the TAT‑QA dataset**:\n",
        "   - Visit [https://nextplusplus.github.io/TAT-QA/](https://nextplusplus.github.io/TAT-QA/) and download the JSON files (`train.json`, `dev.json`, `test.json`).\n",
        "   - Place them in `data/tatqa/`.\n",
        "\n",
        "4. **Set up environment variables**:\n",
        "   Create a `.env` file with your OpenAI API key (if using OpenAI models) or configure your local model paths.\n",
        "\n",
        "5. **Run a baseline experiment**:\n",
        "   ```bash\n",
        "   python run_experiment.py --config configs/baseline.yaml\n",
        "   ```\n",
        "\n",
        "6. **Run evaluation on a subset of the dev set**:\n",
        "   Open the notebook `notebooks/evaluation_demo.ipynb` and execute the cells to see metrics for a few samples.\n",
        "\n",
        "## 📈 Evaluation Metrics\n",
        "\n",
        "We report a comprehensive set of metrics covering both **retrieval** and **generation** aspects:\n",
        "\n",
        "| Metric | Description |\n",
        "|--------|-------------|\n",
        "| **Exact Match (EM)** | Binary: 1 if the normalized answer exactly matches the ground truth. |\n",
        "| **F1 Score** | Harmonic mean of precision and recall at token level. |\n",
        "| **ROUGE‑1 / ROUGE‑2 / ROUGE‑L** | Overlap of unigrams, bigrams, and longest common subsequence. |\n",
        "| **BLEU** | Precision‑based n‑gram overlap (smoothed). |\n",
        "| **Cosine Similarity** | Semantic similarity between sentence embeddings of prediction and ground truth. |\n",
        "| **Precision@k** | Fraction of retrieved top‑k paragraphs that are relevant. |\n",
        "| **Recall@k** | Fraction of relevant paragraphs retrieved in the top‑k. |\n",
        "| **Mean Average Precision (MAP)** | Average precision across all relevant paragraphs. |\n",
        "\n",
        "These metrics are computed per question and then averaged. The evaluation script (`evaluation/evaluate.py`) and the accompanying notebook provide detailed results.\n",
        "\n",
        "## 📊 Preliminary Results\n",
        "\n",
        "On a small subset of the TAT‑QA dev set (5 samples, 1 question each), we obtained the following averages:\n",
        "\n",
        "| Metric          | Value |\n",
        "|-----------------|-------|\n",
        "| **EM**          | 0.20  |\n",
        "| **F1**          | 0.20  |\n",
        "| **ROUGE‑1 F1**  | 0.20  |\n",
        "| **BLEU**        | 0.04  |\n",
        "| **Cosine Similarity** | 0.47 |\n",
        "| **Precision@3** | 0.33  |\n",
        "| **Recall@3**    | 0.81  |\n",
        "| **MAP**         | 2.24* |\n",
        "\n",
        "*MAP currently exceeds 1 due to duplicate retrieved paragraph IDs (e.g., `[4,4,5]`). Deduplication is required to obtain a valid MAP score (expected ≤1).\n",
        "\n",
        "**Observations:**\n",
        "- The retriever consistently finds one relevant paragraph in the top‑3 (precision@3 = 0.33) and, for most samples, all relevant paragraphs (recall@3 = 1.0).\n",
        "- Generation metrics (EM, F1, ROUGE, BLEU) are low for most samples, often zero, despite moderate semantic similarity (cosine similarity). This indicates issues in token‑level matching, likely due to list‑to‑string conversion, punctuation differences, or stopword removal.\n",
        "- Sample 0 (gross margin) achieved perfect EM, F1, ROUGE‑1, and cosine similarity, demonstrating the model's ability to extract exact numeric values when the answer is a simple number.\n",
        "\n",
        "**Current limitations:**\n",
        "- Gold paragraph IDs are heuristically defined by checking if the answer string appears in a paragraph. This may be inaccurate and produce false positives/negatives.\n",
        "- Duplicate retrieved IDs artificially inflate MAP; deduplication is needed.\n",
        "- Token‑level metrics for longer descriptive answers are sensitive to minor variations (e.g., `contract’s` vs `contract's`).\n",
        "\n",
        "Future work will address these issues and expand evaluation to more samples.\n",
        "\n",
        "## 📄 Related Work\n",
        "\n",
        "- **TAT‑QA**: Zhu et al. (2021) – dataset.\n",
        "- **MAPLE**: Bai et al. (2025) – multi‑agent adaptive planning with memory.\n",
        "- **MACT / G‑MACT**: Zhou et al. (2025) – efficient multi‑agent collaboration with tool use.\n",
        "- **ReAcTable**: Zhang et al. (2024) – enhancing ReAct for table QA.\n",
        "- **DocLLM**: Wang et al. (2023) – layout‑aware document LLM.\n",
        "\n",
        "## 👥 Supervision\n",
        "\n",
        "- **Dr. Maryam Lotfi** (Primary Supervisor) – Shahreza Higher Education Center\n",
        "- **Dr. Maryam Hosseini** (Co‑Supervisor) – Shahreza Higher Education Center\n",
        "\n",
        "## 📝 License\n",
        "\n",
        "This project is for academic research purposes. The TAT‑QA dataset is available for non‑commercial use under its own license.\n",
        "\n",
        "## 📬 Contact\n",
        "\n",
        "For questions or collaboration, please contact:  \n",
        "Ghazaleh Keivani Heshajani – [email address]  \n",
        "University of Isfahan, Shahreza Campus\n",
        "\n",
        "---\n",
        "\n",
        "**Note:** This repository is under active development for a master's thesis. Results and code will be updated as the research progresses.\n",
        "```"
      ],
      "metadata": {
        "id": "8M0Hfs0Qq06H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://chat.deepseek.com/share/hymoiu49rlxnwcds24"
      ],
      "metadata": {
        "id": "azO1ngR1Lvsa"
      }
    }
  ]
}